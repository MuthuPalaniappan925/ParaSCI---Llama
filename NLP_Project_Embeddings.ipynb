{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e34eb7",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import FastText\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31182b2",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae4b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ACL = pd.read_csv(\"Data/ACL/train.csv\")\n",
    "data_arxiv = pd.read_csv(\"Data/arXiv/train.csv\")\n",
    "\n",
    "df = pd.concat([data_ACL,data_arxiv],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f585569",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a175572",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = simple_preprocess(remove_stopwords(text))\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    df[i] = df[i].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a589fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df['sentence1'].values.tolist()\n",
    "sentences2 = df['sentence2'].values.tolist()\n",
    "\n",
    "sentences = sentences1 + sentences2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bca931",
   "metadata": {},
   "source": [
    "## Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0cfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e4e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\" \".join(sen) for sen in sentences1[:1000]]\n",
    "test2 =  [\" \".join(sen) for sen in sentences2[:1000]]\n",
    "tests = test + test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31c5ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tfidf = tfidf_matrix.toarray()\n",
    "sample_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce373c3e",
   "metadata": {},
   "source": [
    "### Word2Vec - CBoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68e5d7",
   "metadata": {},
   "source": [
    "The CBOW model learns the embedding by predicting the current word based on its context. CBOW is faster and has better representations for more frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18365b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300  \n",
    "min_word_count = 10\n",
    "num_workers = 4    \n",
    "context = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4a7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=context)\n",
    "cbow = Word2Vec.load(\"cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b37b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(cbow.wv.index_to_key)\n",
    "\n",
    "def get_mean_vector(model, words):\n",
    "    words = [word for word in words if word in vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56b852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural', 1.0),\n",
       " ('learning', 0.5339397192001343),\n",
       " ('deep', 0.5212929844856262),\n",
       " ('vision', 0.49717044830322266),\n",
       " ('recognition', 0.49365249276161194),\n",
       " ('classification', 0.4671529531478882),\n",
       " ('cnns', 0.4524872899055481),\n",
       " ('task', 0.4393136203289032),\n",
       " ('nlp', 0.4372183382511139),\n",
       " ('cnn', 0.4124053120613098)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv.most_similar(cbow.wv['neural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae89bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity for CBoW: 0.8438852429389954\n"
     ]
    }
   ],
   "source": [
    "cbow_score = []\n",
    "for i in range(10):\n",
    "    v1 = get_mean_vector(cbow, sentences1[i])\n",
    "    v2 = get_mean_vector(cbow, sentences2[i])\n",
    "\n",
    "    cbow_score.extend(cbow.wv.cosine_similarities(v1,[v2]))\n",
    "\n",
    "print(f\"Average Cosine Similarity for CBoW: {np.mean(cbow_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee15a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in vocab[:50]:\n",
    "            tokens.append(model.wv[word])\n",
    "            labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=7, n_components=2, init='pca',      n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,6)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fc934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_plot(cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791545be",
   "metadata": {},
   "source": [
    "### Word2Vec - SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56c462",
   "metadata": {},
   "source": [
    " The skip-gram model learns by predicting the surrounding words given a current word. Skipgram works well with small amount of data and is found to represent rare words well.\n",
    "\n",
    " Since the corpus is smaller and contains unique scientific words, Skipgram performs slightly better than CBoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb29af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg = Word2Vec(sentences, sg=1, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=context)\n",
    "sg = Word2Vec.load(\"sg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe36286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural', 1.0),\n",
       " ('deep', 0.7168455719947815),\n",
       " ('convolutional', 0.6565815210342407),\n",
       " ('network', 0.6542420387268066),\n",
       " ('vision', 0.6304519772529602),\n",
       " ('recognition', 0.6088976860046387),\n",
       " ('task', 0.5937486290931702),\n",
       " ('learning', 0.5822420120239258),\n",
       " ('classification', 0.5733896493911743),\n",
       " ('supremely', 0.5633379220962524)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.wv.most_similar(sg.wv['neural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4debe763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity for Skipgram: 0.8762563467025757\n"
     ]
    }
   ],
   "source": [
    "sg_score = []\n",
    "for i in range(10):\n",
    "    v1 = get_mean_vector(sg, sentences1[i])\n",
    "    v2 = get_mean_vector(sg, sentences2[i])\n",
    "\n",
    "    sg_score.extend(sg.wv.cosine_similarities(v1,[v2]))\n",
    "\n",
    "print(f\"Average Cosine Similarity for Skipgram: {np.mean(sg_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c23703",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtsne_plot\u001b[49m(sg)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tsne_plot' is not defined"
     ]
    }
   ],
   "source": [
    "tsne_plot(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71241795",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.save(\"cbow.model\")\n",
    "sg.save(\"sg.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ff0e5",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc578c",
   "metadata": {},
   "source": [
    "Word2Vec only captures the local context of words. During training, it only considers neighboring words to capture the context. GloVe considers the entire corpus and creates a large matrix that can capture the co-occurrence of words within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37222425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_file = 'glove.6B.300d.txt'\n",
    "# word2vec_file = 'glove.6B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_file, word2vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11a68387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "file_name = \"glove.6B.300d.txt.word2vec\"\n",
    "model = KeyedVectors.load_word2vec_format(file_name, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cae92b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = model.key_to_index\n",
    "\n",
    "def glove_mean_vector(model, words):\n",
    "    words = [word for word in words if word in glove_vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c379530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural', 1.0),\n",
       " ('neuronal', 0.6541045904159546),\n",
       " ('neurons', 0.6144998073577881),\n",
       " ('cortical', 0.5799639821052551),\n",
       " ('circuitry', 0.5606817603111267),\n",
       " ('plasticity', 0.5572713017463684),\n",
       " ('pathways', 0.5520570874214172),\n",
       " ('brain', 0.5319003462791443),\n",
       " ('cognitive', 0.5172109007835388),\n",
       " ('neuron', 0.5144911408424377)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(model['neural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dd85d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity for GloVe: 0.8457534909248352\n"
     ]
    }
   ],
   "source": [
    "glove_score = []\n",
    "\n",
    "for i in range(10):\n",
    "    v1 = glove_mean_vector(model, sentences1[i])\n",
    "    v2 = glove_mean_vector(model, sentences2[i])\n",
    "\n",
    "    glove_score.extend(model.cosine_similarities(v1,[v2]))\n",
    "\n",
    "print(f\"Average Cosine Similarity for GloVe: {np.mean(glove_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93e52adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_glove(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in vocab[:50]:\n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=7, n_components=2, init='pca',      n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,6)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "# tsne_plot_glove(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a1514",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec684a7",
   "metadata": {},
   "source": [
    "The working logic of FastText algorithm is similar to Word2Vec, but the biggest difference is that it also uses N-grams of words during training. While this increases the size and processing time of the model, it also gives the model the ability to predict different variations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96666b6",
   "metadata": {},
   "source": [
    "FastText provides a great advantage in obtaining vectors of even words that are not directly in its own vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b19f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext = FastText(sentences, sg=1, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=context)\n",
    "fasttext = Word2Vec.load(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff3a5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 0.9999999403953552),\n",
       " ('modelbased', 0.6032783389091492),\n",
       " ('modeling', 0.4718541204929352),\n",
       " ('blockmodel', 0.46743205189704895),\n",
       " ('metamodel', 0.46408745646476746),\n",
       " ('selectfrommodel', 0.4546191990375519),\n",
       " ('mpn', 0.44286206364631653),\n",
       " ('modelling', 0.4332956075668335),\n",
       " ('facescrub', 0.41293075680732727),\n",
       " ('infersent', 0.40839967131614685)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.wv.most_similar(fasttext.wv['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f6b5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity for FastText: 0.8827802538871765\n"
     ]
    }
   ],
   "source": [
    "fasttext_score = []\n",
    "\n",
    "for i in range(10):\n",
    "    v1 = get_mean_vector(fasttext, sentences1[i])\n",
    "    v2 = get_mean_vector(fasttext, sentences2[i])\n",
    "\n",
    "    fasttext_score.append(fasttext.wv.cosine_similarities(v1,[v2])) \n",
    "\n",
    "print(f\"Average Cosine Similarity for FastText: {np.mean(fasttext_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ea0bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_plot(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.save(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cafaf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e8b5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_paraphrase(idx1, idx2, sent1, sent2, model):\n",
    "    print(\"S1:\", data_ACL['sentence1'][idx1])\n",
    "    print(\"S2:\", data_ACL['sentence2'][idx2])\n",
    "    v1 = get_mean_vector(fasttext, sent1)\n",
    "    v2 = get_mean_vector(fasttext, sent2)\n",
    "    if model == fasttext:\n",
    "        score = model.wv.cosine_similarities(v1,[v2])[0]\n",
    "    elif model == model:\n",
    "        score = model.cosine_similarities(v1,[v2])[0]\n",
    "\n",
    "    if score > 0.75:\n",
    "        print(\"\\nIt is a PARAPHRASE of the first sentence.\")\n",
    "    else:\n",
    "        print(\"\\nIt is NOT A PARAPHRASE of the first sentence. \")\n",
    "    print(f\"Cosine Similarity: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d72eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: for all methods , the tweets were tokenized with the cmu twitter nlp tool .\n",
      "S2: the tweets were tokenized and part-ofspeech tagged with the cmu ark twitter nlp tool and stanford corenlp .\n",
      "\n",
      "It is a PARAPHRASE of the first sentence.\n",
      "Cosine Similarity: 0.8875526189804077\n"
     ]
    }
   ],
   "source": [
    "identify_paraphrase(0, 0, sentences1[0], sentences2[0], fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0e718f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1: for all methods , the tweets were tokenized with the cmu twitter nlp tool .\n",
      "S2: nederhof et al , for instance , show that prefix probabilities , and therefore surprisal , can be estimated from tree adjoining grammars .\n",
      "\n",
      "It is NOT A PARAPHRASE of the first sentence. \n",
      "Cosine Similarity: 0.4238167107105255\n"
     ]
    }
   ],
   "source": [
    "identify_paraphrase(0, 1, sentences1[0], sentences2[1], model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
