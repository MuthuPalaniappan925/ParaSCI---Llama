,sentence1,sentence2,repharsed_text,bleu_score,meteor_score
0,"we run mecab 4 with ipa dictionary 5 on hadoop 6 , an open source software that implemented the map-reduce framework , for parallel word segmenting , part-of-speech tagging , and kana pronunciation annotating .","we run mecab on hadoop 11 , an open source software that implemented the map-reduce framework , for word segmenting and pos tagging the data .","we run mycab with ipa dictionary 3 on hadoop 6, an open source application that implemented the map-reduce framework for parallel word segmentation, part-of-speech tagging and kana",0.255521991160699,0.652338771684063
1,"to accurately represent knowledge , they must represent noun phrases , concepts , and the many-to-many mapping from noun phrases to concepts .","to minimize such confusion , a system must separately represent noun phrases , the underlying concepts to which they can refer , and the many-to-many ¡°can refer to¡± relation between them .","to accurately represent knowledge, they must represent noun phrases, concepts, and the many-to-many mapping from noun phrases to concepts.",0.1631330114188749,0.38720971775634155
2,"for all systems , we trained a 6-gram language model smoothed with modified kneser-ney smoothing using kenlm .",we improved the system combination by adding a 5-grams language model with modified kneser-ney smoothing .,we trained a 5-gram language model with kneser-ney smoothing on the mtp mtg platform.,0.0,0.5896226415094339
3,"in our experiments , we obtain this information from the stanford parser but any other broadly similar parser could be used instead .",for our experiments we use the unlexicalised berkeley parser and the lexicalised form of the stanford parser .,we use the stanford parser as the basis for all we use.,0.0,0.36071428571428577
4,"wedekind achieves this goal by expanding first nodes that are connected , that is , whose semantics is instantiated .","wedekind achieves the reordering by first generating nodes that are connected , that is , whose semantics is instantiated .","we propose a solution with the tuesday mission to expand the first two nodes that are connected, namely the one whose semantics is instantiated.",0.2830789070123405,0.524710086299892
5,"and so our statistical model integrates linguistic , acoustic and situational information .","our method is based on a theoretically clear statistical model that integrates linguistic , acoustic and situational information .",we also model a quantitative quantitative linguistic modelling tool on a single page.,0.0,0.20108108108108108
6,in our approach is to formulate summarization as a phrase rather than sentence .,a key aspect of our approach is the representation of content by phrases rather than entire sentences .,"in our approach, summarization is a natural language in which the context of a sentence is used to make the summarization process a logical sense.",0.0,0.3089685805735189
7,tang et al use enriched task-specific word embeddings and show improvement in a twitter sentiment classification task .,"using a similar corpus , tang et al induced sentiment specific word embeddings , for the twitter domain .",tang et al use enriched task-specific word embeddings and show improvement in a twitter sentiment classification task.,0.0,0.43835978835978845
8,"in this paper , we have presented a case study of the annoying behaviors .","more specifically , we focus on a case study of analyzing annoying behaviors .","in this paper, we present a case study of the annoying behaviors that plagued us.",0.19923405658137924,0.6017439350772683
9,"for simplicity , we use the well-known conditional random fields for sequential labeling .","to predict labels , we train conditional random fields , which are directly optimized for splitting .",we utilize conditional random fields to perform an important skepticism.,0.0,0.3116531165311653
10,this paper describes the process of having humans annotate a corpus of emails .,this paper describes the largest scale annotation project involving the enron email corpus to date .,"in this paper, we address how humans annotate emails.",0.0,0.24
11,"figure 1 shows the topologies of the conventional chain-structured lstm and the treelstm , illustrating the input , cell and hidden node at a time step t .","figure 1 shows the sequence structured lstm of hochreiter and schmidhuber and the treestructured lstm of , illustrating the input , cell and hidden nodes at a certain time step t .","figure 1 shows the topologies of the conventional chain-structured lstm and the treelstm, illustrating the input, cell and hidden node at a time step t.",0.4036473801763577,0.7175887152736234
12,"in this work , we investigate the effectiveness of using domain adaptation .","in our work , we focus on supervised domain adaptation .","in this paper, we focus on the idea of domain adaptation, which is shown to be the first step in the domain evaluation.",0.11556647985416685,0.5050000000000001
13,we then review the observation made by shimoyama and her e-type analysis of ihrc .,we then looked at the argument made by shimoyama and her e-type analysis of ihrc .,"to this end, we use this work to explore the observation of ihrc by shimoyama, shimoyama, nlp, and her post-generative methods.",0.0,0.4048656499636892
14,"because verb usage highly depends on the usage context , which is hard to capture and represent .",the verb choice highly depends on its usage context which is not consistently captured by local features .,"our understanding of verb usage depends on the usage context, which is difficult to capture and represent.",0.0,0.4577348066298343
15,"davidov et al , 2007 ) proposed a method for unsupervised discovery of concept specific relations , requiring initial word seeds .","davidov et al , 2007 ) introduce the use of term frequency patterns for relationship discovery .","davidov et al propose a method for unsupervised discovery of concept specific relations, requiring initial word seed.",0.0,0.3670058139534883
16,"in this paper , we investigate pool-based active learning and joint optimization techniques to collect user feedback for identifying important concepts .","in addition to the joint optimization framework using ilp , we explore pool-based active learning to further reduce the required feedback .","in this paper, we investigated pool-based active learning and joint optimization techniques for identifying important concepts.",0.0,0.38094421582075905
17,"by clustering the semantically related patterns into groups , we can both overcome the data .","second , we cluster the extracted patterns to identify the semantically related patterns .",we use the clustering process to build semantically related structures to gain data.,0.0,0.3425655976676385
18,"in this study , we have extended the techniques of automatic humor recognition to different types of humor as well as different languages .","in addition , we constructed a chinese dataset to evaluate the generality of the method performance on humor recognition against different languages .","we will analyze the best method for automated humor recognition in this study, et al.",0.0,0.23751395089285715
19,galley et al proposed an mt model which produces target parse trees for string inputs in order to exploit the syntactic structure of the target language .,galley et al describe an algorithm for inducing a string-to-tree grammar using a parallel corpus with syntax trees on target side .,galley et al proposed a single parsing structure which is based on two types of data.,0.0,0.23772609819121443
20,"the problem of polarity classification has been studied in detail by wilson et al , who used a set of carefully devised linguistic features .","the problem of polarity classification has been studied in detail by wilson , wiebe , and hoffmann , who used a set of carefully devised linguistic features .",wilson et al used carefully devised linguistic features to identify semantic differences between different domains in language.,0.09173866800550408,0.2601273148148148
21,previous works showed that conditional random fields can outperform other sequence labeling models like memms in abbreviation generation tasks .,previous works proved that conditional random fields can outperform other sequence labeling models like memms in abbreviation generation tasks .,previous works showed that conditional random fields can outperform other sequence labeling models like memms in abbreviation generation tasks.,0.8739351325046804,0.9999375
22,"predicate vectors are learned from the contexts of preceding arguments , and are required to contribute to the prediction of upcoming arguments .","similarly , predicate vectors are learned from the contexts of preceding arguments , and are required to contribute to the prediction of upcoming arguments .","a predicate vector is expected from the contexts of preceding arguments, and is required to contribute to the prediction of upcoming arguments.",0.6416598266162529,0.8018574297188756
23,shorter sentences that convey the same meaning is a challenging problem .,"processing long , complex sentences is challenging .",shorter sentences which convey the same meaning is the problem.,0.0,0.18072289156626503
24,recent work on temporal resolution focuses primarily on news .,previous work on adapting temporal taggers primarily focus on scaling up to more languages .,recent work on temporal resolution has focused primarily on news.,0.0,0.2920471841704718
25,"in this paper , we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution .","to address the problem above , in this paper , we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution .","in this paper, we propose a simple and novel way to automatically generate large-scale pseudo training data for zero pronoun resolution.",0.6120554790781674,0.7383587237711985
26,we give an extended lexrank with integer linear programming to optimize sentence selection .,we then use an extension of the lexrank algorithm to rank sentences .,we give an extended lexrank with integer linear programming that optimizes sentence selection.,0.0,0.19083969465648856
27,for details on the computation of this code length see .,details on the computation of this code length are given in .,for details on the computation of this code length see.,0.6381941796682007,0.7521527129370266
28,"towards this overall goal , we describe the construction of a resource that contains more than 160 , 000 document pairs that are known to talk about the same events .",our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events .,we present a comprehensive study of the structure of a document gathered into a broader graphical framework to improve.,0.0,0.175
29,"for example , cut can be used in the sense of ¡° cutting costs , ¡± which carries with it restrictions on instruments , locations , and so on that somewhat overlap with eliminate .","for example , cut can be used in the sense of ¡°cutting costs , ¡± which carries with it restrictions on instruments , locations , and so on that somewhat overlap with eliminate as in ¡°eliminating costs.¡±","in some cases, cut can be used in the sense of ° cutting costs,  which carries with it restrictions on instruments, locations, and so on that somewhat overlap with eliminate.",0.665478713196997,0.8119565217391306
30,"however , to train maxent , we do not need manually labeled training data .",we adopt this method as well but with no use of manually labeled data in training .,"we have learned the correct level of training from our 3rd platform, as the training data are in place for the best speed.",0.0,0.2247191011235955
31,"to adapt to user and domain changes , we performed an application-oriented analysis of different online algorithms .","focusing on the adaptability to user and domain changes , we report the results of comparative experiments with two online algorithms and the standard batch approach .",we have performed an application-oriented analysis of online algorithms and modified them into a language.,0.0,0.19734019734019734
32,we proposed to solve the semantic textual similarity task .,"we proposed udl , a model for estimating sentence pair semantic similarity .",we proposed a semantic textual similarity task in order to solve it all.,0.0,0.3254877014418999
33,the classifier used was svm light described in using a linear kernel .,the taxonomy kernel was trained using the svm 脨 脴 package .,"as a classifier, the classifier used was svm light described in using a linear kernel.",0.0,0.24
34,named entity recognition ( ner ) is the first step for many tasks in the fields of natural language processing and information retrieval .,named entity recognition ( ner ) is a well-known problem in nlp which feeds into many other related tasks such as information retrieval ( ir ) and machine translation ( mt ) and more recently social network discovery and opinion mining .,named entity recognition ( ner ) is a complex complex task in natural language processing.,0.09220524648994927,0.22548978478835421
35,we use latent semantic analysis to perform this representational transformation .,"as an alternative , we apply latent semantic analysis to compute a reduced-rank representation .",we use latent semantic analysis to perform this representational transformation.,0.18049661583447188,0.43472183393905506
36,"in standard penntreebank ( cite-p-24-1-21 ) evaluations , our parser achieves a significant accuracy improvement ( + 1 . 8 % ) .","in standard ptb evaluation , our parser achieved a 1.8 % accuracy improvement over the parser of cite-p-24-1-1 , which shows the effect of combining search and learning .",our standard penntreebank cite-p-24-1-1-21 is capable of achieving the best possible accuracy improvement ( + 1. 8 % ) in a lexicalized text.,0.1464786690444522,0.24013176905738887
37,"as a classification problem , we focused on identification and examination of various linguistic features such as verb class , tense , aspect , mood , modality , and experience .","we formulate the problem as a classification task using various linguistic features including tense , mood , aspect , modality , experiencer , and verb classes .",the aim of our work was to train a range of cognitive constructs to optimize the linguistic features of the word.,0.0,0.14037735849056604
38,we describe a highly efficient monotone search algorithm with a complexity .,we described a highly efficient monotone search algorithm .,the general rule-based optimization algorithm uses a high quality monotone search algorithm.,0.2044800736021839,0.5148936170212766
39,"in this paper , we present three different approaches for the textual semantic similarity task of semeval 2012 .",in this paper we describe the three approaches we submitted to the semantic textual similarity task of semeval 2012 .,"in this paper, we use the semantic similarity of the semantic semantics semantic task semeval 2012.",0.0,0.5476361245592015
40,we also show how the proposed architecture can be applied to domain adaptation .,"finally , we also show how this architecture can be used for domain adaptation .",we show that domain adaptation is a possible feature based on a set of computational features.,0.0,0.28051900584795325
41,"hierarchical convolutional neural network ( shcnn ) , which integrates local and more global representations of a message , is first presented to estimate the conversation-level similarity between closely posted messages .","in the first stage , we propose the siamese hierarchical convolutional neural network ( shcnn ) to estimate conversation-level similarity between pairs of closely posted messages .",the hierarchical convolutional neural network ( shcnn ) is first presented to estimate the conversation-level similarity between closely posted messages.,0.37722087904694357,0.7413352272727273
42,"tresner-kirsch ( 2012 ) use the logarithm of the frequency for some experimental runs , reporting that it improved accuracy .","vogel and tresner-kirsch ( 2012 ) use the logarithm of the frequency for some experimental runs , reporting that it improved accuracy in some cases .",tresner-kirsch ( 2012 ) uses the logarithm for experimental run data for different metrics.,0.10399549604157803,0.4059212054830894
43,"in addition , we demonstrate that topicvec can derive coherent topics based only on one document , which is not possible for topic .","in addition , we illustrate with an example that our method can generate coherent topics even based on only one document .","as a result, we show that topicvec can produce coherent topics in a single text, based only on one document, which is impossible for topicvec to distinguish.",0.0,0.3957262086251002
44,predicting the subject of a disease / symptom .,identifying whether the subject has a disease/symptom .,predicting the subject of a disease / symptom.,0.45936133207830593,0.3896604938271605
45,xiong and zhang employ a sentence-level topic model to capture coherence for document-level machine translation .,"xiong and zhang attempt to improve lexical coherence via a topic-based model , using a hidden topic markov model to determine the topic in the source sentence .",xiong and zhang use sentences to capture a syntactic structure for document-level machine translation.,0.0,0.23642322097378274
46,information exchange through social media concerning various health challenges has been extensively studied .,analysis of social media content for health has been a topic of wide interest .,social media is widely used to discuss health challenges in healthcare and its determinants.,0.0,0.248
47,"to ensure that task-specific features of given task do not exist in the shared space , we exploit the concept of adversarial training into shared feature space .","inspired by adversarial networks , we incorporate adversarial training into shared space to guarantee that specific features of tasks do not exist in shared space .","for this purpose, we exploit the notion of adversarial training into the shared feature space to ensure that task-specific features of given task do not exist in the shared space.",0.16995165296029044,0.6876262678857809
48,"in particular , we carefull y studied the fastus system of hobbs et al , who have clearly and eloquently set forth the advantage s of this approach .","in particular , we carefully studied the fastus system of hobbs et al , who have clearly and eloquently set forth the advantages of this approach .","in particular, we study the fastus system of hobbs et al, who have clearly and eloquently set forth the advantage s of this approach.",0.7539221180326288,0.9622233179925488
49,we evaluate the output of the unsupervised pos tagger as a direct replacement for the output of a fully supervised pos tagger for the task of shallow parsing .,"finally , building on this promising result we use the output of the unsupervised pos tagger as a direct replacement for the output of a fully supervised pos tagger for the task of shallow parsing .",we evaluate the output of a fully supervised pos tagger as a direct replacement for the output of a fully supervised pos tagger for the task of shallow parsing.,0.6354031470803295,0.7331260655902117
50,"punyakanok et al , 2005 , typically involves multiple stages to 1 ) parse the input , 2 ) identify arguments , 3 ) classify those arguments , and then 4 ) run inference to make sure the final labeling for the full sentence does not violate any linguistic constraints .","punyakanok et al , 2005a , typically involves multiple stages to 1 ) parse the input , 2 ) identify arguments , 3 ) classify those arguments , and then 4 ) run inference to make sure the final labeling for the full sentence does not violate any linguistic constraints .","punyakanok et al proposed a reorganization for the first stage of the work between the three phases : 1 ) parse the input, 1 ) identify arguments, 3 ) classify",0.21165320016999065,0.2930296354794463
51,"on three narrow domain translation tasks , caused little increase in the translation time , and compared favorably to another alternative retrieval-based method with respect to accuracy , speed , and simplicity of implementation .","it also causes little increase in the translation time , and compares favorably to another alternative retrieval-based method with respect to accuracy , speed , and simplicity of implementation .","after all, we compared the speed and simplicity of the implementation by reducing the translation lag, with the proposed retrieval-based method being used in a single project.",0.0,0.372
52,in this exchange share one belief that we have not represented .,the user and advisor in this exchange share one belief that we have not represented .,in this exchange share one belief that we have not represented.,0.7165313105737893,0.769008190883191
53,the semantic textual similarity task examines semantic similarity at a sentence-level .,the task of semantic textual similarity is aimed at measuring the degree of semantic equivalence between a pair of texts .,semantic textual similarity task examines semantic similarity at a sentence-level.,0.0,0.26601562500000003
54,ucca ¡¯ s approach that advocates automatic learning of syntax from semantic supervision stands in contrast to the traditional view of generative grammar ( cite-p-11-1-7 ) .,ucca¡¯s representation is guided by conceptual notions and has its roots in the cognitive linguistics tradition and specifically in cognitive grammar ( cite-p-11-3-6 ) .,the ucca  s approach that advocates automatic learning of syntax from semantic supervision stands in contrast to the traditional view of generative grammar ( cite-p-11-1-7 ).,0.09881511482966875,0.22716229348882414
55,"by processing n-best lists or lattices , which allows us to consider both the segmented and desegmented output .","in this paper , we expand our translation options by desegmenting n-best lists or lattices .",us also see which output is segmented or desegmented.,0.0,0.0974025974025974
56,"based on the structured perceptron , we propose a general framework of ¡° violation-fixing ¡± perceptrons for inexact search with a theoretical guarantee for convergence .",we have presented a unifying framework of ¡°violation-fixing¡± perceptron which guarantees convergence with inexact search .,for the model we propose a general framework of ° violation-fixing  perceptrons for inexact search with a theoretical guarantee for convergence.,0.0,0.4811809220352483
57,cite-p-12-5-6 improved the crf method by employing the large margin method .,cite-p-12-5-7 reported a pos tagger based on cyclic dependency network .,the cite-p-12-5-4 improves the crf method by employing the large margin method.,0.1513851459876605,0.044642857142857144
58,"we term the ¡° word generalization ¡± problem , which refers to how children associate a word such as dog with a meaning at the appropriate category level .","we use the term ¡°word generalization¡± to refer to this problem of associating a word with the meaning at an appropriate category level , given some sample of experiences with the word .","we term the ° word generalization problem, which refers to how children associate a word such as dog with a meaning at the appropriate category level.",0.0,0.4407841466013211
59,"in nature , the entities themselves tend to be complex and expressed as noun phrases containing multiple modifiers , giving rise to examples like oxygen depletion in the upper 500 m of the ocean or timing and magnitude of surface temperature evolution in the southern hemisphere in deglacial proxy records .","variables tend to be complex rather than atomic entities and expressed as noun phrases containing multiple modifiers , e.g . oxygen depletion in the upper 500 m of the ocean or timing and magnitude of surface temperature evolution in the southern hemisphere in deglacial proxy records .","in nature, the entities themselves tend to be complex and expressed as noun phrases containing multiple modifiers, giving rise to examples like oxygen depletion in the upper 500 m of the ocean or timing and magnitude of surface temperature evolution",0.5624424086766802,0.7107471071920228
60,huang et al and huang et al mainly focused on the generative hmm models .,"huang , harper , and wang and huang , eidelman , and harper mainly focused on the generative hmm models .",huang et al and huang et al have developed a novel kneser-ney standardised kneser-ney models for the generative hmm models.,0.20217803037339238,0.3571428571428571
61,"we compare our proposed model 3 with three existing models including cnn , minie , and clausie by corro and gemulla .","baselines we compare our method with both competitive neural and non-neural models , including rnnoie , openie4 , 5 clausie , and props .","we compare our proposed model 3 to three existing models including cnn, minie, and clausie by corro and gemulla.",0.0,0.3356135842766858
62,"in this paper , we propose to employ statistical machine translation to improve question retrieval and enrich the question representation with the translated words from other languages .","in this work , we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages .","in this paper, we propose to employ statistical machine translation to improve question retrieval and enrich the question representation with the translated words from other languages.",0.13008376498509774,0.3903976254264403
63,"models , along with two non-composition models , namely the adjective and the noun models , are used to explain the systematic variance in neural activation .","furthermore , vector-based semantic models can explain a significant portion of systematic variance in the observed neural activity .","the adjective and noun models are used as the, to explain the systematic variance in neural activation.",0.0,0.5043497172683776
64,"in chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .",we utilize this substring based approach and obtain this local tagging information by labeling on the substring of the full character sequence .,"in chinese abbreviation prediction, we use a substring tagging strategy to generate local substring tagging candidates.",0.0,0.15555555555555556
65,stance classification is the task of automatically identifying users ’ positions about a specific target from text ( cite-p-18-1-10 ) .,"stance classification is the task of determining from text whether the author of the text is in favor of , against , or neutral towards a target of interest .",stance classification is a common problem in stance classification.,0.0,0.152281746031746
66,we experimentally evaluated the test collection for single document summarization contained in the rst discourse treebank distributed by the linguistic data consortium 1 .,we conducted an experimental evaluation on the test collection for single document summarization evaluation contained in the rst discourse treebank distributed by the linguistic data consortium 3 .,we evaluated our test collection for one document summarization found in the rst discourse treebank.,0.16636072310752337,0.4285525037534222
67,"in order to evaluate the performance of our tensorbased factorization model of compositionality , we make use of the sentence similarity task for transitive sentences , defined in grefenstette and sadrzadeh .","in order to evaluate the performance of our new co-compositional model with prototype projection and word representation learning algorithm , we make use of the disambiguation task of transitive sentences developed by grefenstette and sadrzadeh .",a large sample of the structure between sentences may be obtained from grefenstette and sadrzadeh.,0.054213599977348995,0.2065716911764706
68,"in this paper , it is possible to provide theoretical guarantees for distributed online passive aggressive learning .",in this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available .,this paper provides a comprehensive understanding of the benefits of passive aggressive learning.,0.0,0.15564853556485356
69,"following , we describe the algorithm in a deductive system .","following chiang , we describe our algorithms in a deductive system .","following, we describe the algorithm in a deductive system.",0.4412739850976205,0.8289915966386556
70,"system performance is evaluated on newstest 2011 using bleu , meteor , and ter .",performance is measured in terms of bleu and ter computed using the multeval script .,the nlp was used to evaluate the overall system performance on cnn 2014.,0.0,0.1342281879194631
71,"in order to cluster lexical items , we use the algorithm proposed by brown et al , as implemented in the srilm toolkit .","to determine the word classes , one can use the algorithm of brown et al for finding the classes .","for this task, we use the brown et al et al algorithm, as implemented in the sri model.",0.0,0.3701492537313433
72,machinery can be obtained without going beyond the power of mildly context-sensitive grammars .,we thus show that these analyses can be obtained without requiring power beyond mildly context-sensitive grammars .,"without extra-sense grammars, no work is achieved for the word-sequence model.",0.0,0.12048192771084339
73,"experiments have been conducted on four publicly available datasets , including three synthetic conversation datasets and one real conversation dataset from reddit .",experiments were conducted with four publicly available datasets of conversations from reddit and irc channels .,"experiments have been conducted on four publicly available datasets, including three synthetic conversation datasets and one real conversation dataset from reddit.",0.1392908359945467,0.5319938635126441
74,"in this section , we generalize the ideas regarding network-based dsms presented in , for the case of more complex structures .","in this section , we summarize the main ideas of dsms that were proposed in for building semantic networks , which are extended here for the creation of affective networks .","in this section, we generalize the ideas regarding the dsm implementations presented in, for the case of more complex structures.",0.14616966435958637,0.42310522442972776
75,"in this paper , we conducted a systematic comparative analysis of language in different contexts of bursty topics , including web search , news media .","in this paper , we have studied and compared how the web content reacts to bursty events in multiple contexts of web search and online media .","for the second article, we used a systematic comparative analysis of language in different contexts of bursty topics, including web search, news media.",0.0,0.3506944444444444
76,gaustad showed that human-generated pseudo-words are more difficult to classify than random choices .,gaustad showed that evaluations using pseudowords can over-estimate the accuracy of a word sense disambiguation system on real data .,gaustad showed that pseudo-words are more difficult to classify than random choices than human-generated words.,0.0,0.22755102040816327
77,the generation of referring expressions is a core ingredient of most natural language generation systems .,the generation of referring expressions is an integral part of most natural language generation systems .,the generation of referring expressions is the most important factor in natural language generation and thus a critical problem.,0.31569611706824424,0.5843075992743401
78,evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system .,our evaluation demonstrates that scisumm achieves higher quality summaries than a state-of-the-art multidocument summarization system ( cite-p-15-3-4 ) .,evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system.,0.0,0.3287760416666667
79,huang et al utilise a bi-directional lstm with a sequential conditional random layer using a gazetteer and senna word embeddings to obtain superior performance .,huang et al use a bilstm with a crf layer in addition to making use of explicit spelling and context features along with word embeddings .,"huang et al use the senna standard toolkit as the model for developing short, short-short word embeddings.",0.12680761723756567,0.2964426877470356
80,all experiments used the europarl parallel corpus as sources of text in the languages of interest .,"for the english-german experiments , the translation system was trained and tested using a part of the europarl corpus .",all experiments used europarl to build data on languages of interest on the same theme.,0.0,0.1530612244897959
81,"in this work , we use fasttext for training embeddings .","additionally , we use averaged word embeddings .",we use fasttext to train and generate the embeddings on english and english platforms.,0.0,0.3627873563218391
82,which can be seen as a compromise of the hierarchical phrase-based model and the tree-to-string model .,"in some sense , our model can be seen as a compromise between the hierarchical phrase-based model and the tree-to-string model , specifically",phrase-based model can be viewed as a compromise of hierarchical phrase-based model and tree-to-string model.,0.19770074816697586,0.5281514698555057
83,it was followed by schwenk who applied neural network for language modeling in large scale vocabulary speech recognition and obtained a noticeable improvement in word error rate .,"his work has been followed by schwenk , who has shown that neural network language models actually work very well in the state-of-theart speech recognition systems .","schwenk, a small business model with a full neural network in several languages, showed the decrease in word error rates.",0.0,0.2195303072496055
84,"brockett et al treat error correction as a translation task , and solve it by using the noisy channel model .","similar to our approach , brockett et al view error correction as a machine translation problem .",brockett et al used this tool to solve the problem using a noisy channel model.,0.0,0.33872720685907504
85,"when dealing with highly inflected or agglutinative languages , as well as analytic languages , of which chinese is the focus of this article .","but a number of augmentations and changes become necessary when dealing with highly inflected or agglutinative languages , as well as analytic languages , of which chinese is the focus of this article .","we focus on chinese language, particularly in the languages where chinese is the topic.",0.0,0.19604037267080746
86,deeb-rnn achieves better performance as compared to the state-of-the-art methods in terms of both recall and f1-measure .,experimental results show that the deebrnn model outperforms both feature-based and representation-based state-of-the-art methods in terms of recall and f1-measure .,"deeb-rnn and the f1-measure program are trained to measure the recalls, recalls and f-measures.",0.0,0.20799457994579945
87,label propagation is a semi-supervised algorithm which needs labeled data .,traditional label propagation is a graph-based semi-supervised learning approach with a single view .,"label propagation is a classifier, or classification tool that can perform the same task with your own input.",0.1326475916741222,0.350076103500761
88,"for each domain , we formulate the review spam detection tasks for multiple domains , e . g . , hotel , restaurant , and so on , as a multi-task learning problem .","we formulate review spam detection for multiple domains ( e.g. , hotel and restaurant ) as a multi-task learning problem .","despite this, we set out a collection of the review spam detection tasks for both domains, and our services with their different features.",0.0,0.3321428571428571
89,birke and sarkar proposed the trope finder system to recognize verbs with non-literal meaning using word sense disambiguation and clustering .,birke and sarkar propose a minimally supervised algorithm for distinguishing between literal and non-literal usages of verbs in context .,"birke and sarkar proposed a recurrent trope finder system, which uses the word sense disambiguation to target other words.",0.0,0.2542841348811498
90,ibm model 4 is essentially a better model .,our model is also easier to understand than ibm model 4 .,ibm is an ibm model that looks better than it is without its proper implementation.,0.0,0.30000000000000004
91,"given a set of question-answer pairs as the development set , we use the minimum error rate training algorithm to tune the feature weights 位 m i in our proposed model .","to set the model parameters 胃 , we used the minimum error rate training algorithm to maximize the f-measure of the 1-best alignment of the model on a development set consisting of sentence pairs with manually generated alignments .",we use the minimum error rate training algorithm to tune feature weights m i in our proposed model.,0.10587154824246131,0.2833370560643288
92,jiang and zhai introduce a general instance weighting framework for model adaptation .,jiang and zhai recently proposed an instance re-weighting framework to take domain shift into account .,jiang and zhai introduced a general instance weighting framework for model adaptation.,0.0,0.32554847841472045
93,"when the physician ' s plan is deficient , several problems are generally detected , and thus multiple critiques are independently produced .","our experience with a critiquing system shows that when the system detects problems with the user 's performance , multiple critiques are often produced .","the physician' s plan is more likely to be misguided, where multiple critiques are n tainted, unless the critique is deficient.",0.0,0.2525
94,in this paper we study the problem of interpreting and verbalizing visual information using abstract scenes .,in this paper we study the problem of interpreting visual scenes and rendering their content using natural language .,in this paper we present a numerical report on the concept of interpreting and verbalizing visual information using abstract scenes.,0.16078626955573463,0.5324074074074074
95,"in this paper , we focus on designing a review generation model that is able to leverage both user and item information .","in this paper , we argue that the extra aspect ( opinion ) information extracted using these previous works can effectively improve the quality of generated reviews .",this work is intended to make use of the emoji toolkit for the review generation system.,0.0,0.16728624535315986
96,our system outperforms a comparable well-published system and a previously published form of our system .,"the system outperforms a comparable publicly available system , as well as a previously published form of our system .",our system outperforms a comparable well published system and a previously published form of our system.,0.4909136024426773,0.7023723194861701
97,we use the maximum entropy model as implemented in the stanford corenlp toolset .,we use a maximum entropy classifier which allows an efficient combination of many overlapping features .,we implement the maxim entropy model to optimize the performance in stanford.,0.0,0.09554140127388534
98,there is a method to automatically learn the weights but it requires reference phrase pairs not easily available in resource constrained scenarios like ours .,there is a method to automatically learn the interpolation weights but it requires reference phrase pairs which are not easily available .,there is an automatic method to automatically learn the weights but it requires reference phrase pairs not easily available in resource constrained scenarios like ours.,0.4332008456263299,0.7949597663139331
99,as word vectors the authors use word2vec embeddings trained with the skip-gram model .,"the word vectors are learned using a skip-gram model with negative sampling , implemented in the word2vec toolkit .",the resulting results depend on the word2vec embeddings trained on the skip-gram model.,0.0,0.2304804804804805
100,"in this paper , we propose bridge correlational neural networks which can learn common representations for multiple views .","to this end , we propose bridge correlational neural networks ( bridge corrnets ) which learn aligned representations across multiple views using a pivot view .","in this paper, we discussed the use of a bridge correlational neural network for processing swipe-like views.",0.0,0.35115800476921566
101,"in machine translation , improved language models have resulted in significant improvements in translation performance .","large language models have been shown to improve quality , especially in machine translation .","in machine translation, improved language models have resulted in significant improvements in translation performance.",0.0,0.45580900989289513
102,this paper describes our participation in the language identification in code-switched data task at codeswitch 2014 .,following work has been described in the first shared task on language identification in code-switched data held at emnlp 2014 .,this paper describes our participation in the language identification in code-switched data task at codeswitch 2014.,0.22805511105403836,0.5461165048543689
103,"to obtain a vector representation of a sentence and score each target word in the sentence , and for heterographic puns , we computed the semantic similarity between cluster center vectors of each sentence .","for homographic puns , we calculated the semantic similarity between sense vectors of each target word in the sentence to obtain its sentence vector and score each target word .","for these vectors, we present a semantic similarity between cluster center vectors of a sentence and sequence words.",0.0,0.396041624158335
104,"charniak , 2000 ) extends pcfg and achieves similar performance to .",charniak 2000 ) describes a different method which achieves very similar performance to .,"charniak, 2000 ) extends pcfg and achieves similar performance to.",0.2293287319577502,0.5434782608695652
105,"in this paper , we give an overview of our participation in the timeline generation task of semeval-2015 .","in this paper , we presented an overview of our participation in the timeline generation task of semeval-2015 .","in this paper, we present a brief overview of our participation in a 2014 semeval timeline generation task.",0.4284945090100314,0.7602094240837695
106,"in this framework , review feature words and opinion words are organized into categories in a simultaneous and iterative manner .","we propose an iterative reinforcement framework , and under this framework , review feature words and opinion words are organized into categories in a simultaneous and iterative manner .","for this purpose, review feature words and opinion words are organized into categories in a simultaneous and iterative manner on a large page.",0.5770462999792318,0.658282161052244
107,one key reason is that the objective functions of topic models do not correlate well with human judgements .,chang et al stated that one reason is that the objective function of topic models does not always correlate well with human judgments .,some major reason is that objective functions of topic models do not correlate well with human judgements.,0.19011980122747407,0.6349477682811017
108,"djuric et al propose an approach that learns low-dimensional , distributed representations of user comments in order to detect expressions of hate speech .","djuric et al were the first to propose a self-taught learning strategy in the context of hateful speech detection , where they simultaneously learn low-dimension representations of documents and words in a common vector space .",djuric et al propose a model that learns low-dimensional representations of user comments in order to detect expressions of hate speech.,0.0,0.322543352601156
109,sen proposed a latent topic model to learn the context entity association .,sen modeled the topic coherence as the groups of co-occurring entities .,sen proposed a latent topic model to learn a semantic association between context entities.,0.0,0.3024390243902439
110,"each essay was represented through the sets of features described below , using term frequency and the liblinear scikit-learn implementation of support vector machines with ovr , one vs .","we used term frequency weighting scheme and the liblinear scikit-learn implementation of support vector machines with ovr , one vs .","each essay was represented through the sets of features described below, using term frequency and the liblinear scikit-learn implementation of support vector machines with ovr, one vs.",0.4959135018706127,0.8213540785839111
111,"in this paper , we present a neural keyphrase extraction framework for microblog posts that takes their conversation context into account , where four types of neural encoders , namely , averaged embedding , rnn , attention , and memory .","in this paper , we present a neural keyphrase extraction framework that exploits conversation context , which is represented by neural encoders for capturing salient content to help in indicating keyphrases in target posts .","in this paper, we present an interactive neural keyphrase extraction framework, which takes conversation context into account, with three types of neural encoders, averaged embedding, rnn, attention,",0.24100801068158104,0.4176553545845272
112,most spoken languages use brahmiderived scripts .,all languages use different brahmidescended scripts .,most spoken languages use brahmi-derived scripts to express phrases that appear more accurate.,0.0,0.40990259740259744
113,"to address the generalization concern , we propose a method inspired by yarowsky .",our method is based on a decision list proposed by yarowsky .,in this paper we propose a method that is inspired by yarowsky.,0.0,0.4730983302411874
114,"in recent years , many accurate phrase-structure parsers have been developed , .","recently , many accurate statistical parsers have been proposed for english , for japanese ) .","in recent years, many accurate phrase-structure parsers have been developed.",0.0,0.45022035256410264
115,"to manipulate strings , current methods usually have no access to most information available in decoding phase .",current smt systems usually decode with single translation models and can not benefit from the strengths of other models in decoding phase .,"since we use string manipulation, the current methods don’t have access to much information available in decoding phase.",0.11417084020749106,0.24563318777292575
116,goldwater et al explored a bigram model built upon a dirichlet process to discover contextual dependencies .,"goldwater et al justified the importance of context as a means of avoiding undersegmentation , through a method based on hierarchical dirichlet processes .",goldwater et al use a dirichlet process to learn dependencies that apply to multiple languages in the project.,0.0,0.2988696808510638
117,"in our study , we explore the use of generalized lexical features for predictive opinion analysis .",our experiment focuses on investigating aspects of predictive opinions by learning lexical patterns and comparing them with judgment opinions .,"in this paper, we demonstrate that our findings represent a highly competitive picture of our data in our study.",0.0,0.0746268656716418
118,three systems are unsupervised and relied on dictionary-based similarity measures .,the three systems are unsupervised and relied on dictionary-based similarity measures .,"the translational system is based on a single dictionary-based similarity measure, which is unsupervised and used by a standard classification model.",0.0,0.5852417302798982
119,named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance .,"named entity recognition is the task of finding entities , such as people and organizations , in text .",named entity recognition is a key task in our understanding of lexicon recognition.,0.1479257593879102,0.2875844594594595
120,paul s . jacobs phred : a generator for natural language interfaces .,paul s. jacobs phred : a generator for natural language interfaces,paul s. jacobs phred is a natural language interface which aims to train a number of parameters in a mt system.,0.18331704949485053,0.643724173553719
121,"as a building block : we model the left and right sequences of modifiers using rnns , which are composed in a recursive manner to form a tree .","in this work , we use the rnn abstraction as a building block , and recursively combine several rnns to obtain our tree representation .",we used the rnns to structure the left and right sequences of modifiers.,0.0,0.23951840150286058
122,"we perform standard phrase extraction to obtain our synthetic phrases , whose translation probabilities are again estimated based on the single-word probabilities pfrom our translation model .","finally , we compute the translation probabilities according to the estimated co-occurrence counts , using the standard training method in phrase-based smt .",this works based on a single-word probabilities model with lmlc.,0.0,0.04587155963302752
123,"in this paper , 4 word boundary tags are employed : b ( beginning of a word ) , m ( middle part of a word ) , e ( end of a word ) .","in this paper , 4 word boundary tags are employed : b ( beginning of a word ) , m ( middle part of a word ) , e ( end of a word ) and s ( single character ) .","in this paper, word boundary tags were used for b ( beginning of a word ), m ( middle part of a word ), e ( end of a word ).",0.6536060298777379,0.7615112160566707
124,"in the first approach , heuristic rules are used to find the dependencies or penalties for label inconsistency are required to handset ad-hoc .","however , in the first approach , heuristic rules are used to find the dependencies or penalties for label inconsistency are required to handset ad-hoc .",heuristic rules are used to detect dependencies ( cite-p-21-8 ) for label inconsistency.,0.1733028084586718,0.35823780325246596
125,"while data sparsity is a common problem of many nlp tasks , it is much more severe for sentence compression , leading cite-p-12-3-4 to question the applicability of the channel model for this task altogether .","data sparsity is the bane of natural language processing ( nlp ) ( cite-p-15-5-2 , cite-p-15-3-7 ) .",we also think that data sparsity is a well-known problem in many nlp tasks.,0.0,0.2519774011299435
126,"from the oceanic language family , our model achieves a cluster purity score over 91 % , while maintaining pairwise recall over 62 % .","on the larger oceanic data , our model can achieve cluster purity scores of 91.8 % , while maintaining pairwise recall of 62.1 % .","we use the oceanic language family to generate a cluster purity score over 91 %, while maintaining pairwise recall over 62 %.",0.24050508677015364,0.5072360447707992
127,"to address the first processing stage , we build phrase-based smt models using moses , an open-source phrase-based smt system and available data .","for this experiment , we train a standard phrase-based smt system over the entire parallel corpus .","for the nlp process, we use moses, an open-source phrase-based smt system and available data to map the initial processing stage.",0.0,0.356638418079096
128,"s酶gaard and goldberg , 2016 ) showed that inducing a priori knowledge in a multi-task model , by ordering the tasks to be learned , leads to better performance .","s酶gaard and goldberg , 2016 ) showed that a higher-level task can benefit from making use of a shared representation learned by training on a lower-level task .","sgaard and goldberg, 2016 ) show that inducing a priori knowledge in a multi-task model by ordering the tasks to be learned leads to better performance.",0.15224470383400482,0.46875
129,"however , our results suggest that the tensor-based methods are more robust than the basic hal model .",results suggest that the tensor-based methods we propose are more robust than the basic hal model in some respects .,the tensor-based method is more robust than the basic hal model.,0.3008141382064201,0.5671057162534435
130,we use the simplified factual statement extractor model 6 of heilman and smith .,"to do the task of simplification , we use the simplified factual statement extraction toolkit .","the factual statement extractor is based on the slatan formula, introduced by the new model.",0.0,0.26483781918564525
131,"character-level models obtained better accuracies than previous work on segmentation , pos-tagging and word-level dependency parsing .",experimental results show that the character-level dependency parsing models outperform the word-based methods on all the data sets .,"character-level models obtained better accuracies than previous work on segmentation, pos-tagging and word-level dependency parsing.",0.0,0.2268026004728132
132,"users also subjectively rate the rl-based policy on average 10 % higher , and 49 % higher .",users also subjectively rate the rl-based policy on average 10 % higher .,users subjectively rate the rl-based policy on average 10 % higher and 49 % higher than previous studies.,0.5049567629086866,0.8660130718954249
133,we propose a novel approach that is based on pattern discovery and supervised learning to successfully identify erroneous / correct sentences .,"in this paper , we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models .",we proposed a novel method to identify sentences that resembl erroneous to correct it with one word.,0.0,0.3549382716049383
134,we also explore bi-lstm models to avoid the detailed feature engineering .,we also explore bi-lstm models with fewer features .,"to avoid this from being a novel set, we propose to increase tagging.",0.0,0.10416666666666666
135,english tweets are identified using a compression-based language identification tool .,english tweets are identified automatically using a compression-based language identification tool .,these words are identified using the compression-based language identification tool.,0.3809694917244035,0.6545430672268908
136,zhang et al propose a simplified neural network which contains only one hidden layer and use three different pooling operations .,zhang et al explore a shallow convolutional neural network and achieve competitive performance .,zhang et al propose a single layer-based neural network in which multiple-stricken and sub-spinal messages are automatically pooled to achieve the same results.,0.0,0.5111111111111112
137,named entity recognizer ( ner ) on upper case text can be improved by using a mixed case .,"named entity recognition on mixed case text is easier than on upper case text , where case information is unavailable .",named entity recognizer ( ner ) on upper case text can be improved using a mixed case.,0.15837807011854305,0.3975070078129659
138,because common topics in weakly-correlated collections are usually found in the tail of the document-topic distribution of a sufficiently large set of topics .,"topic correlations in weakly-related collections typically lie in the tail of the topic distribution , where they would be overlooked by models unable to fit large numbers of topics .","as a result, common topics in weakly-correlated collections are often found in the tail of the document-topic distribution of a sufficiently large set of topics.",0.1755573895006456,0.42508417508417506
139,universal dependencies is a cross-linguistically consistent annotation scheme for dependency-based treebanks .,universal dependencies is a framework for cross-linguistically consistent treebank annotation .,"universal dependencies are annotations, and they are essentially the same class as dependencies.",0.0,0.21929824561403505
140,"likewise , skadi艈a et al and skadi艈a et al argue that the advantages of comparable corpora in machine translation are considerable and more beneficial than those of parallel corpora .",skadi艈a et al and skadi艈a et al argue that the advantages of comparable corpora in machine translation are considerable and more beneficial than those of parallel corpora .,both skadia et al and skadia et al argue that the advantages of comparable corpora in machine translation are considerable and more beneficial than those of parallel corpora.,0.833078701050083,0.9250563288340458
141,we also use a simple bagging technique to deal with the sparsity of boundary tags .,"for large datasets , we use an ensemble technique inspired by bagging .","for boundary tags, we also use a simple bagging technique.",0.0,0.3717766176237937
142,convolution tree kernel defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees .,convolution tree kernel defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees to express the respective distance in the feature space .,the convolution tree kernel defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees.,0.6933906702450128,0.75625
143,"when a peco-structured query is formulated , it is matched against the peco elements in the documents .",one can then match the peco elements in the query to the elements detected in documents .,"when a peco-structured query is formulated, it is matched against the peco elements in the documents.",0.23909453161355018,0.3414915890549419
144,stochastic models have been widely used in pos tagging task for simplicity and language independence of the models .,stochastic models have been widely used in pos tagging for simplicity and language independence of the models .,stochastic models are widely used in pos tagging tasks for simplicity and language independence of the models.,0.6899302125555485,0.8859592013888888
145,the selectional preference distribution was defined in terms of selectional association measures introduced by resnik over the noun classes automatically produced by sun and korhonen .,the selectional preference distribution is defined in terms of selectional association measure introduced by resnik over the noun classes automatically produced by sun and korhonen .,we define the selectional preference distribution in terms of selectional association measures introduced by resnik over the noun classes automatically produced by sun and korhonen.,0.7432254773174188,0.9607076923076924
146,"for clustering , we use the implementation from the cluto software package .","particularly , we used a partitioning algorithm of the cluto library for clustering .",the model is structured into a cluto clustering model which consists of the clustering packages.,0.0,0.30027386541471046
147,as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models .,the results demonstrate the superiority of a clustered approach over both traditional prototype and exemplar-based vector-space models .,the words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector space models.,0.2046592065585361,0.6320203400121434
148,word embeddings have been proven helpful in many nlp tasks .,all of them have proven that word embedding is helpful in nlp tasks .,word embeddings have been proven to be effective in nlp.,0.0,0.4178459705049902
149,"using manually compiled document-independent features , we develop a novel summary system called priorsum , which applies the enhanced convolutional neural networks to capture the summary .","to this end , we develop a novel summarization system called priorsum to automatically exploit all possible semantic aspects latent in the summary prior nature .","this project uses document-independent features developed with our toolkit, priorsum, to capture the summary from the dataset.",0.0,0.15748031496062992
150,svm was used since it is known to perform well for sentiment classification .,we choose to use svm since it performs the best for sentiment classification .,"we used a standard svm feature in sentiment classification, which is known for svm.",0.0,0.3377407300948549
151,"in the cross-domain setting , and a traditional ilp method does not work well in the in-domain setting .","on the other hand , a deletion-based method does not face such a problem in a cross-domain setting .","more importantly, it does not work in the in-domain setting.",0.0,0.31281365005018397
152,like ours is the first proposal of its kind .,"as far as we know , our work is the first of its kind .",like ours is the first proposal of its kind.,0.2304318198457308,0.537176724137931
153,statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model .,but a bare-bones statistical model is still useful in that it allows us to quantify precise improvements in performance upon the integration of each specific cue into the model .,statistical model is also useful in its ability to quantify precise improvements in performance upon the integration of each specific cue into the model.,0.6101290968468593,0.7776104578513986
154,"in the pattern selection process , we propose to capture and exploit these relationships using pattern-based entailment graphs .",we propose to exploit entailment relationships holding among re patterns by structuring the candidate set in an entailment graph .,we propose to utilize pattern-based entailment graphs as the results of the pattern selection process.,0.0,0.3583386479591837
155,we apply domain adversarial training only on the topic inputs from learned topic representations .,we consider the domain adversarial training network on the user factor adaptation task .,we apply domain adversarial training only on topic inputs from learned topic representations.,0.0,0.3650793650793651
156,"in addition , we extend the sick dataset to include unscored fluency-focused sentence comparisons .","in addition , we extend the sick dataset to include unscored fluency-focused sentence comparisons and we propose a toy metric for evaluation .",we extend the sick dataset to include unscored fluency-focused sentence comparisons.,0.36925046456270755,0.5436643835616438
157,we obtained these scores by training a word2vec model on the wiki corpus .,we mainly used pre-trained embeddings of words from the conll 2017 shared task trained on word2vec .,we trained a model using the wiki corpus.,0.0,0.15432098765432098
158,demberg uses a fourth-order hidden markov model to tackle orthographic syllabification in german .,"demberg applies a fourth-order hmm to the syllabification task , as a component of a larger german text-tospeech system .",demberg uses the dsp system with a hidden markov model to tackle orthographic syllabification in german.,0.0,0.20304568527918782
159,"extending the results of lapata et al , we confirmed that cooccurrence frequency can be used to estimate the plausibility of an adjective-noun pair .",lapata et al demonstrated that the cooccurrence frequency of an adjective-noun combination is the best predictor of its rated plausibility .,"as a result of this study, we also estimate the plausibility of a nlp relationship in the literature by showing that the nlp model can be used as a metric of the cooccurrence frequency of",0.08202649282493453,0.28055555555555556
160,"we have introduced semeval-2018 task 5 , a referential quantification task of counting events and participants in local news articles with high ambiguity .","this paper discusses semeval-2018 task 5 : a referential quantification task of counting events and participants in local , long-tail news documents with high ambiguity .","we introduce semeval-2018 task 5, the referential quantification task of counting events and participants in local news articles with high ambiguity.",0.5056430229372445,0.7325630274744818
161,"translations are filtered using a measure of similarity to the original words , based on latent semantic analysis scores .",words or expressions are aligned using a word similarity model based on a combination of latent semantic analysis and semantic distance in the wordnet knowledge graph .,we use semantic analysis to evaluate translations using latent semantic analysis scores in which they compare to the original words.,0.0,0.17045454545454547
162,and we plan to explore this possibility in future work .,we plan to explore this possibility in future work .,we intend to explore a few of these options as they are discussed in future work.,0.17694975149532557,0.6284569902727446
163,"in this paper , we proposed a lifelong learning approach to sentiment classification .",this paper proposes a novel lifelong learning ( ll ) approach to sentiment classification .,"in this paper, we proposed a lifelong learning approach to sentiment classification.",0.32575662810116884,0.7205058516833988
164,"backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting cite-p-21-3-0 .","contrary to cite-p-21-3-0 , we proved that these models can be exactly collapsed into a single backoff language model .","backoff language models can be effectively and exactly collapsed into a single normalized backoff model, contradicting cite-p-21-3-0.",0.3847897411368105,0.6378832940211261
165,while is measured across ten randomized embedding spaces trained on the training data of the ptb ( determined using language modeling splits ( cite-p-16-1-18 ) ) .,stability is measured across ten randomized embedding spaces trained on the training portion of the ptb ( determined using language modeling splits ( cite-p-16-1-18 ) ) .,while is measured across ten randomized embedding spaces trained on the training data of the ptb ( determined using language modeling splits ( cite-p-16-1-18 ) ).,0.8787142254774354,0.9256888888888889
166,experimental results show that our model significantly outperforms the direct transfer method .,experimental results show that our method consistently outperforms various baselines across languages .,experimental results show that our model significantly outperforms the direct transfer method.,0.32523403430389786,0.576923076923077
167,"to address this drawback , ranking models were proved to be a useful solution , .","to address this drawback , ranking models were proved to be useful solutions , .","to address this drawback, ranking models were proved to be a useful solution to this problem.",0.5974970909115025,0.7482993197278912
168,"however , multi-word translation units have been shown to improve the quality of smt dramatically .",using multi-word phrases instead of individual words as the basic translation unit has been shown to increase translation performance .,"although, multi-word translations have been shown to improve the quality of smt significantly.",0.0,0.32371794871794873
169,"as table 7 shows , our system clearly outperforms the system proposed by silfverberg and hulden with regard to f1-score on tags .","in addition , our system substantially improves upon the baseline presented by silfverberg and hulden .",we propose a second f1-score system called rebt on tags that matches the standard f1score model proposed by silfverberg and hulden on tags.,0.10401577613691954,0.3777939747327502
170,a chain can be defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes .,a pattern is defined as a path between a verb node and any other node in the dependency tree passing through zero or more intermediate nodes .,"we can define a chain that is a unified network with node, verbs and the dependency tree.",0.0,0.3048703551826382
171,"for annotation tasks , snow et al showed that crowdsourced annotations are similar to traditional annotations made by experts .","snow et al showed , however , that crowdsourced annotations can produce similar results to annotations made by experts .",snow et al compared crowdsourced annotations with traditional annotations made by experts.,0.24806575277784984,0.5111398963730571
172,our analysis shows that the high-performance of the acm comes .,our analysis shows that the high-performance of the acm lies in the asymmetry of the model .,we suggest that the high-performance of the acm is predicted by the nfl model.,0.32821002882577516,0.3475896531452087
173,"in the second phase , it selects an optimal substitute for each given word from the synonyms according to the context .","in the second phase , it selects an optimal substitute for each given word from the synonyms according to the context in math-w-3-4-0-74 .","in the second phase, it selects an optimal substitute for each given word from the synonyms according to the context.",0.6688643306977606,0.9231977915132994
174,"in reviews , customers might express different sentiment towards various aspects of a product or service .",reviews depict sentiments of customers towards various aspects of a product or service .,"in reviews, customers express different sentiments towards different aspects of a product or service.",0.4169392927528885,0.7382726108718426
175,state-of-the-art smt models achieve excellent results by extracting phrases to induct the translation rules .,"with large amounts of data , phrase-based translation systems achieve state-of-the-art results in many typologically diverse language pairs .","to achieve excellent results, we use advanced smt models to extract phrases from the phrase translation algorithm.",0.0,0.13157894736842105
176,"this approach was pioneered by galley et al with numerous variants in subsequent research , usually referred to as tree-to-tree , tree-to-string and string-to-tree , depending on where the analyses are found in the training data .","this approach was pioneered by galley et al , and there has been a lot of research since , usually referred to as tree-to-tree , treeto-string and string-to-tree , depending on where the analyses are found in the training data .","galley et al used numerous variants in subsequent research that are usually referred to as tree-to-tree, tree-to-string and string-to-tree.",0.12908788487362818,0.3554415954415954
177,"in this work , we take a more direct approach and treat a word type and its allowed pos tags .","in this paper , we make a simplifying assumption of one-tag-per-word .","in this work, we have looked at a set of word types and its permitted pos tags.",0.0,0.5530265748031497
178,discourse referents showed that script knowledge is a highly important factor in determining human discourse expectations .,our hypothesis is that script knowledge may be a significant factor in human anticipation of discourse referents .,discourse referents demonstrate that script knowledge is a important factor in determining the discourse expectations of people.,0.0,0.4935720844811754
179,"in our experiments , this method is shown to be very effective to boost the performance of keyphrase extraction .",we find that topic-sensitive propagation can largely help boost the performance .,we present a model for our recurrent method that shows it is very effective to boost the performance of keyphrase extraction.,0.0,0.3931623931623932
180,this makes the information such as cur- 5 related work benamara and dizier present the cooperative question answering approach which generates natural language responses for given questions .,related work benamara and dizier present the cooperative question answering approach which generates natural language responses for given questions .,"in this paper, we develop a cooperative answer answering approach which creates natural language responses for selected questions.",0.19835441454182887,0.484
181,and we plan to explore such semantic signals in future work .,we plan to incorporate such signals in future work .,we propose to explore such semantic signals in future work.,0.39458812555917666,0.7425742574257427
182,semantic textual similarity is the task of finding the degree of semantic equivalence between a pair of sentences .,the semantic textual similarity is a core problem in the computational linguistic field .,semantic textual similarity is a standard task in semantic analysis.,0.2905374198590231,0.4632802025919856
183,"in the semi-supervised adaboost algorithm , we investigate two boosting methods in this paper .","based on this semi-supervised boosting algorithm , we investigate two boosting methods for word alignment .","in the adaboost algorithm, we test the proposed bi-directional boosting approach for the mt algorithm.",0.0,0.26483781918564525
184,"phrase structure trees in ctb have been semi-automatically converted to deep derivations in the ccg , lfg , and hpsg formalisms .","phrase structure trees in ctb have been semiautomatically converted to deep derivations in the ccg , lfg , tag and hpsg formalisms .","the phrase structure trees are semi-automatically converted to deep derivations in ccg, lfg, and hpsg formalisms.",0.416980616086774,0.7426432311602412
185,cahill et al reported an application of the pcfg approximation technique in lfg parsing and the recovery of long distance dependencies on the f-structures .,cahill et al present a method to automatically obtain approximations of ldd resolution for lfg resources acquired from a treebank .,cahill et al demonstrated a similar use of the pcfg approximation technique for lfg parsing.,0.0,0.37398373983739835
186,the iterative scaling algorithm combined with monte carlo simulation is used to train the weights in this generative model .,"the obtained average observations are set as constraints , and the improved iterative scaling algorithm is employed to evaluate the weights .","for generating weights, we use the monte carlo simulation to produce three genuine and anitized the weights.",0.0,0.2908986175115207
187,"the idea is that documents are represented as random mixtures over latent topics , where each topic is characterized by a distribution over words .","the basic idea behind topic models is that documents are mixtures of topics , where a topic is a probability distribution over words .","this is to understand the fact that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.",0.17966302274629617,0.6450309149696523
188,"and an analysis of the results shows that the generalization methods of resnik and li and abe appear to be overgeneralizing , at least for this task .","an analysis of the results has shown that the other approaches appear to be overgeneralizing , at least for this task .","we observed that it appears the generalization methods of resnik and li and abe are overgeneralizing, at least for this task.",0.322888846243622,0.5715012450132535
189,and our results suggest that the attention of generative networks can be successfully biased to look at sentences relevant to a topic .,our method works by modifying the attention mechanism of a pointer-generator neural network to make it focus on text relevant to a topic .,"in our study, ltd looks at the importance of generative networks in determining sentence categories.",0.0,0.10729613733905581
190,noise-contrastive estimation has been a successful alternative to train continuous space language models with large vocabularies .,noise-contrastive estimation has been successfully adopted for training neural language models with large vocabularies .,noise-contrastive estimation has long been a key feature in introducing large vocabularies and a large number of long-standing loopwork model and a large number of long-lived tv networks,0.0,0.35119569300112685
191,we use approximate randomization for significance testing .,we compute statistical significance using the approximate randomization test .,we use approximately randomized probability testing to measure significance patterns for each of these variables.,0.0,0.45244512899499423
192,we look at word level perplexity with respect to the word frequency .,"in this work , we present , at word level , the correlation between perplexity and word frequency .",we approach word level perplexity in determining the word frequency.,0.0,0.4212454212454212
193,"kurokawa et al show that for an english-to-french mt system , a translation model trained on an english-to-french data performs better than one trained on french-to-english translations .",kurokawa et al showed that french-to-english smt systems whose translation models were constructed from human translations from french to english yielded better translation quality than ones created from translations in the other direction .,"kurokawa et al show that for an english-to-french mt system, a translation model trained on an english-to-french dataset performs better than one trained on french-to",0.0,0.3255731258366801
194,"following the common practice of adaptation research on this data , we take the union of bn and nw as the source domain and bc , cts and wl as three different target domains .","following the common practice of domain adaptation research on this dataset , we use news as the source domain and bc , cts , wl as three different target domains .","following the common practice of adaptation research on this data, we take the union of bn and nw as the source domain and bc, cts and wl as three different target domains.",0.537428567412207,0.7762738853503186
195,experiments were run with a variety of machine learning algorithms using the scikit-learn toolkit .,all the experiments are developed using the scikit-learn machine learning library .,the experiment is run using a variety of machine learning algorithms.,0.0,0.35532407407407407
196,"considering that users do not know in which terms the categories are expressed , they might query the same concept .","considering that users do not know in which terms the categories are expressed , they might query the same concept by a paraphrase .","thus, users do not know in which categories are expressed, they may query the same concept.",0.44410633990831605,0.675531914893617
197,"however , recnns require a predefined topological structure , like parse tree , to encode .","however , recnns need a given external topological structure , like syntactic tree .","recnns are highly complex with the corresponding layering, e.g. a predefined topological structure, like parse tree, can be detected in a dictionary.",0.09865637517691195,0.3816670701202292
198,"we use an iterative rule distillation process to effectively transfer rich structured knowledge , expressed in the declarative first-order logic language , into parameters of general neural networks .","specifically , we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks .",we use a third-order method to deliver a multidimensional mt-gram model based on a non-irrational way to convey information in an unbiased manner.,0.0,0.10822510822510824
199,"on the one hand , we do not expect such pairs to occur in any systematic pattern , so they could obscure an otherwise more systematic pattern .","on the one hand , we do not expect such pairs to occur in any systematic pattern , so they could obscure an otherwise more systematic pattern in the high pmi bins .","on the other hand, we do not expect such pairs to occur in any systematic pattern, so they could obscure an otherwise more systematic pattern.",0.7389973333630001,0.8262108262108262
200,"in the previous task , we introduce a task and a dataset consisting in a set of crowdsourced human ratings of spatial similarity for object pairs .","to evaluate the quality of the spatial representations learned in the previous task , we introduce a task consisting in a set of 1,016 human ratings of spatial similarity between object pairs .","in the previous task, we introduce a task and a dataset consisting of crowdsourced human ratings of spatial similarity for object pairs.",0.3831814547021198,0.6096962616822431
201,"evaluation on a standard data set shows that our method consistently outperforms the best performing previously reported method , which is supervised .",evaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task .,our evaluation shows that a logical and balanced approach is consistently outperforming the best performing previously reported method.,0.0,0.3738693467336683
202,"by using entice , we are able to increase nell ’ s knowledge density by a factor of 7 . 7 .",we find that entice is able to significantly increase nell ’ s knowledge density by a factor of 7.7 at 75.5 % accuracy .,we use a 3-gram -based entice model and learn about nell’ s knowledge density.,0.0,0.3546722009365688
203,"we obtain useful information from wikipedia by the tool named java wikipedia library 2 , which allows to access all information contained in wikipedia .","for retrieving the discussion pages , we use the java wikipedia library , which offers efficient , databasedriven access to the contents of wikipedia .",we extract data from the java wikipedia library 2 by using a library of data that can help to revert to wikipedia information.,0.1157064510816097,0.2552208835341366
204,"in bohnet et al , the goal is to improve parsing accuracy for morphologically rich languages by performing morphological and syntactic analysis jointly instead of in a pipeline .","bohnet et al presented a joint approach for morphological and syntactic analysis for morphologically rich languages , integrating additional features that encode whether a tag is in the dictionary or not .","in bohnet et al, the aim of the project is to increase parsing accuracy for morphologically rich languages by performing morphological and syntactic analysis jointly.",0.19509920410791545,0.4761904761904763
205,chen and ng further extend the study of zhao and ng by proposing several novel features and introducing the coreference links between zps .,"on the base of zhao and ng , chen and ng further investigate their model , introducing two extensions to the resolver , namely , novel features and zero pronoun links .",chen and ng further extend the study of zhao and ng by proposing several novel features and introducing the coreference links between zps.,0.19071831531293626,0.42949969951923084
206,"besides , riezler et al and zhou et al proposed the phrase-based translation models for question and answer retrieval .","in the same space , riezler et al develop smt-based query expansion methods and use them for retrieval from faq pages .",riezler et al also proposed phrase-based translation models for question retrieval.,0.0,0.24338624338624337
207,and is available for download at github . com / sdl-research / hyp .,hyp is available for download at github.com/sdl-research/hyp .,it also is available for download at github.com / sdl-research / hyp.,0.8507331335123524,0.7911164465786316
208,"here too , we used the weka implementation of the na茂ve bayes model and the svmlight implementation of the svm .","for our learners , we used the na茂ve bayes implementation in the weka machine learning toolkit , a support vector machine , 3 and the crf implementation in mallet .",we used the weka implementation of the nave bayes model and the svmlight implementation of the svmlm.,0.0,0.23846418732782368
209,"in this method , dual decomposition is used as a framework to take advantage of both hpsg parsing and coordinate structure .","in this paper , we present a statistical analysis model for coordination disambiguation that uses the dual decomposition as a framework .","in this method, dual decomposition is used to take advantage of both hpsg parsing and coordinate structure.",0.0,0.2908986175115207
210,dependencies are incorporated into the crf model via a ( relatively ) straightforward feature extraction scheme .,the dependencies were included in the crf model using a relatively straightforward feature expansion scheme .,dependencies are incorporated into the crf model using a relatively straightforward feature extraction scheme.,0.4984390309259902,0.6848069026456677
211,phrase table pruning is the technique of removing ineffective phrase pairs from a phrase table to make it smaller while minimizing the performance degradation .,"phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller , ideally removing the least useful phrases first .",phrase table pruning is a type of research focused on addressing the problem of separating the ineffective phrase pairs from the corresponding phrase table and minimizes grading.,0.1504843536148922,0.46840876572031015
212,discourse cohesion model can help better capture discourse structure information .,identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance .,discourse cohesion model can help better capture discourse structure information.,0.0,0.10948905109489049
213,"by using well calibrated probabilities , we are able to estimate the sense priors effectively .","in this paper , we explore the estimation of sense priors by first calibrating the probabilities from naive bayes .","we employ well calibrated probabilities to estimate the sense priors and i.e, a well calibrated probability tool.",0.0,0.29344252124821646
214,morphologically rich languages ( mrl ) are languages for which important information concerning the syntactic structure .,"morphologically rich languages ( mrl ) are languages in which much of the structural information is contained at the word-level , leading to high level word-form variation .",morphologically rich languages ( mrl ) are languages where important information concerning the syntactic structure is presented ( cite-p-21-17-1-5 ).,0.26800510323761967,0.36781758675249804
215,"in our experiments , we used the kyoto university text corpus and kyoto university web document leads corpus as manually tagged corpora .","in our experiments , we used the kyoto university text corpus 11 and the kyoto university web document leads corpus 12 as japanese treebanks .",our experiment was based on the kyoto university text corpus.,0.1068827436571301,0.2976032838983051
216,textual units is represented as a rooted tree whose nodes correspond to the minimum textual units .,hence a document is represented as a nested tree where each node of the outer tree corresponds to an inner tree .,textual units are based on the minimum textual units.,0.0,0.04807692307692308
217,word embeddings represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words .,"word embeddings , as a low-dimensional continuous vectors of words are regarded to be an efficient representations of word semantics .",word embeddings represent each word as a low-dimensional vector with certain characteristics of semantic similarity of words.,0.0,0.4574275362318841
218,jokinen et al use a topic model based on a tree organisation of domain knowledge to detect topic shifts .,jokinen et al combine a manually built tree for main topics with an n-gram model for topic shifts .,jokinen et al implemented a target tree-organizing task to generate topic shifts from the domain knowledge layer.,0.0,0.33581686642911124
219,"using the expectation maximization algorithm with viterbi decoding , we adopt the iterative parameter estimation procedure to solve the maximum likelihood estimation problem .","as there is no closed form solution for the maximum likelihood estimate , we resort to iterative training via the em algorithm .",we use the viterbi decoding algorithm to create the iterative parameter estimation procedure.,0.0,0.18099547511312217
220,"snow et al used dependency parses to automatically learn such patterns , which they used to augment wordnet with new hypernym relations .","snow et al use syntactic path patterns as features for supervised hyponymy and synonymy classifiers , whose training examples are derived automatically from wordnet .","snow et al used the clusters of dependency structures to train a number of languages with lts, while not utilizing the default default parses.",0.0,0.2528660866737133
221,"riaz and girju propose cluster sentences into topic-specific scenarios , and then focus on identifying causal relations between events and building a dataset of causal text spans headed by a verb .","in , the authors first cluster sentences into topic-specific scenarios , and then focus on building a dataset of causal text spans , where each span is headed by a verb .","riaz and girju proposed cluster sentences into topic-specific scenarios, and then focus on identifying causal relations between events and building a dataset of causal text spans headed by a verb.",0.5633809697190241,0.6805268595041322
222,"for a dense annotation , a major downside is the limitation that events and time expressions must be in the same or in adjacent sentences .",a drawback of the previous annotation works is the limitation that only links between expressions in the same or in succeeding sentences are annotated .,"for dense annotation, an important downside is the limitation that events and time expressions must be in the same or in adjacent sentences.",0.2589451141492935,0.4944378698224852
223,"we perform all our experiments on the english section of the conll-2012 corpus , which is based on ontonotes .","in our experiments , we rely on the english portion of the conll-2012 dataset .",we train a graphical representation of the conll-2012 corpus on ontonotes using the full english text.,0.0,0.2631578947368421
224,however their generalization on unseen text is relatively poor comparing with models that exploit syntactic tree .,"however , the current recursive architecture is limited by its dependence on syntactic tree .",our generalization on unseen texts is relatively poor compared with models that exploit syntactic tree.,0.0,0.295364238410596
225,"novelty mining studies on the chinese language have been performed on topic detection and tracking , which identifies and collects relevant stories on certain topics .","although novelty mining studies have mainly been conducted on the english language , studies on the chinese language have been performed on topic detection and tracking .","novelty mining studies on the chinese language have been conducted on topic detection and tracking, which identifies and collects relevant stories on certain topics.",0.43598465625857263,0.5700123915737298
226,"we investigate a new way for extracting hypernymy relations , exploiting the text layout which expresses hierarchical relations .",our aim is to improve the relation extraction task by considering both the plain text and the layout .,"we present a research on a new way of extracting hypernymy relations, exploiting the text layout which expresses hierarchical relations.",0.0,0.155440414507772
227,instances are chosen to be labelled depending on their similarity with the seed instances and are added in the seed set .,the promoted instances are either added to the initial seed set or used to replace it .,instances are selected to be labelled according to their similarity with the seed instances and are added in the seed set.,0.0,0.36314285714285716
228,"subject and object can not easily be identified compared to english , while their detections are the key process to generate correct english word orders .","the detection of subjects and objects from japanese sentences is more difficult than that from english , while it is the key process to generate correct english word orders .","subject and object can not easily be identified compared to english, while their detections are the key process to generate correct english word order.",0.2802436832260487,0.5705835593378846
229,"where math-w-3-3-0-1 is the number of words in the corpus , and math-w-3-3-0-12 is a predetermined window size .","for each word math-w-2-6-2-17 , we construct a vector math-w-2-6-2-24 of size math-w-2-6-2-27 , where math-w-2-6-2-30 is the size of the lexicon .","where math-w-3-3-0-1 is the number of words in the corpus, and math-w-3-3-0-12 is a predetermined window size.",0.0,0.22123893805309733
230,"since our multilingual skip-gram and cross-lingual sentence similarity models are trained jointly , they can inform each other through the shared word embedding layer .","even though they are related tasks , multilingual skip-gram and cross-lingual sentence similarity models are always in a conflict to modify the shared word embeddings according to their objectives .","since our multilingual skip-gram model is trained jointly, our feature-based model can inform each other through the shared embedding layer.",0.0,0.21618150684931506
231,"the ability to identify paraphrase , in which a sentences express the same meaning of another one but with different words , has proven useful for a wide variety of natural language processing applications .","alternative expressions of the same meaning , and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications .",ldl used for processing natural language data on translation algorithms to extract information on the semantic spectrum of different types of words.,0.0,0.30028583856335045
232,"particularly , zeng et al proposed a piecewise convolutional neural network architecture , which can build an extractor based on distant supervision .","to address the issue of lack of data , zeng et al incorporate multi-instance learning with a piece-wise convolutional neural network to extract relations in distantly supervised data .","particularly, zeng et al proposed a piecewise convolutional neural network architecture which can build an extractor based on distant supervision.",0.12114135894482563,0.3312720848056538
233,"word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words ( white , 2004 ; zhang and clark , 2015 ; de gispert et al. , 2014 ; bohnet et al. , 2010 ; filippova and strube , 2007 ; he et al. , 2009 ) , which is practically relevant to text-to-text applications such as summarization ( cite-p-11-3-2 ) and machine translation ( cite-p-11-1-1 ) .",word ordering is a fundamental problem in nlp and has been shown to be np-complete in discourse ordering ( cite-p-16-1-1 ) and in smt with arbitrary word reordering ( cite-p-16-3-6 ) .,"word ordering is a common classification classification task, derived from the general use of a variety of classification methods such as classifying words as words, grammatiques, and verbs as texts.",0.06321342041944326,0.10869565217391304
234,"balamurali et al , have shown that use of senses in places of words improves the performance of indomain sa significantly .","balamurali et al , have shown that senses are better features than words for in-domain sa .",balamurali et al use senses in places of words to optimize reactivity of indomain sa.,0.0,0.3755585074266393
235,we have used a simplified version of the miniature language acquisition task proposed by feldman et al .,this domain is a simplification of the miniature language acquisition task proposed by feldman et al .,"as a model, we have provided a simplified version of the language acquisition task outlined in the feldman et al report.",0.0,0.5068181818181818
236,experimental studies demonstrate the effectiveness of our approach .,results demonstrate the effectiveness and generality of our approach .,we studied the evaluation of a single language based on a four-gram model.,0.0,0.14423076923076922
237,itg constraints are not sufficient on the canadian hansards task .,with the extended itg constraints the coverage improves significantly on both tasks .,the two constraints of a language translation system are not sufficient for hansards.,0.0,0.11450381679389314
238,"pgf is the backend format to which grammatical framework ( gf , ranta , 2004 ) grammars are compiled .","pgf is a simple “ machine language ” , to which the much richer gf source language is compiled by the gf grammar compiler .","pgf is the backend format to which grammatical frameworks are compiled by an ngrammatical framework ( gf, ranta, 2004 ) or a grammatical framework",0.0,0.2964143426294821
239,"elsner and charniak , elsner and charniak present a combination of local coherence models initially provided for monologues showing that those models can satisfactorily model local coherence in chat dialogues .","elsner and charniak , elsner and charniak are presenting a combination of local coherence models initially provided for monologues showing that those models can satisfactorily model local coherence in chat dialogues .",elsner and charniak showed that it does not seem to be a good way to model slurs of a chat dialog.,0.0,0.26725806451612905
240,"by parallelizing the clustering algorithm , we successfully constructed a cluster gazetteer with up to 500 , 000 entries .","we enabled such large-scale clustering by parallelizing the clustering algorithm , and we demonstrate the usefulness of the gazetteer constructed .","we used an online tool to generate a cluster gazetteer with up to 500,000 entries.",0.0,0.12195121951219513
241,"using previously proposed automatic measures , we find that we can not reliably predict human ratings .","therefore , we build a ranking model which successfully mimics human judgments using previously proposed automatic measures .","using previously proposed automatic measures, we found that we can not reliably predict human ratings.",0.24722771625388107,0.4983240223463687
242,"we encode a relatively rich lexical semantic structure for nouns based on the notion of qualia structure , described by pustejovsky , 1989 pustejovsky , 1991 .","briscoe et al and copestake illustrate some lexical entries with the qualia structure following pustejovsky and aniek , pustejovsky , 1989 pustejovsky , 1991 .","as a result, we encode a relatively rich lexical semantic structure for nouns based on the notion of qualia structure, described by pustejovsky, 1989 pustejovsky, 1991",0.18628206061552108,0.4111165127207909
243,le and mikolov introduce paragraph vector to learn document representation from semantics of words .,le and mikolov presented the paragraph vector algorithm to learn a fixed-size feature representation for documents .,le and mikolov propose a novel semantic analysis of text using a paragraph s.,0.0,0.3042328042328043
244,"on these collections , it is necessary to have topic models that are aligned across languages .",we introduce a polylingual topic model that discovers topics aligned across multiple languages .,we also offer the opportunity to model languages that are aligned on their content model.,0.0,0.2112676056338028
245,"in the parliament domain , this means ( and is translated as ) “ report .","in the parliament domain , this means ( and is translated as ) “ report . ”","in a literature translation format, this means /translates as ” report.",0.0,0.4618473895582329
246,the grammar matrix is couched within the head-driven phrase structure grammar framework .,hpsg is a syntactic theory based on lexicalized grammar formalism .,our grammar matrix is couched within the head-driven phrase structure grammar framework.,0.0,0.13392857142857142
247,"as an example of these probabilistic methods , stolcke et al apply a hmm method to the switchboard corpus , one that exploits both the order of words within utterances and the order of dialogue acts over utterances .","stolcke et al apply a somewhat more complicated hmm method to the switchboard corpus , one that exploits both the order of words within utterances and the order of dialogue acts over utterances .","as an example of these probabilistic methods, stolcke et al employ a hmm method to the switchboard corpus, one that exploits both the order of words within utterances and the",0.5943021411987796,0.6969099276791584
248,table 9 : f-score of different types of reparandums .,table 1 : different types of disfluencies .,table 9 : f-score of different types of reparandums.,0.0,0.6233062330623306
249,evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement .,experiment results reveal that our weighted evaluation metrics gives more reasonable and distinguishable scores and correlates well with human judgement .,evaluated evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement.,0.611047986960551,0.7630270633971292
250,previous works in qa have shown that these relations can help us answer certain kinds of questions .,rst tells us that sentences with discourse relations are related to each other and can help us answer certain kinds of questions .,"previously, the results of the previous works in qa show that these relations can help us answer certain kinds of questions from existing knowledge.",0.28326302347195154,0.4607526691022595
251,we solve this sequence tagging problem using the mallet implementation of conditional random fields .,we design our model for ssl as a natural semisupervised extension of conventional supervised conditional random fields .,we implemented conditional random fields in the mallet loop using the webl classifier.,0.0,0.25340909090909086
252,"categorial grammar provides a functional approach to lexicalised grammar , and so , can be thought of as defining a syntactic calculus .","categorial grammar provides a functional approach to lexicalised grammar , and so can be thought of as defining a syntactic calculus .","categorial grammar provides a functional approach to lexicalised grammar, and therefore can be thought of as defining a syntactic calculus.",0.8725129388059689,0.9541331684188827
253,parallel or comparable corpora have also been explored for unsuperwised wsd .,several studies have shown encouraging results for wsd based on parallel corpora .,unsuperwised wsd is a widely studied wsd corpus.,0.0,0.11904761904761907
254,we utilize maximum entropy model to design the basic classifier used in active learning for wsd and tc tasks .,we utilize a maximum entropy model to design the basic classifier for wsd and tc tasks .,we use the maximum entropy model to develop the active learning model for this task.,0.17103943246074768,0.40726130469720206
255,"in recent work , recurrent neural network language models have produced stateof-the-art perplexities in sentence-level language modeling , far below those of traditional n-gram models .","in recent years , neural lms have become the prominent class of language modeling and have established state-of-the-art results on almost all sufficiently large benchmarks .","in recent work, recurrent neural networks language models have produced stateof-the-art perplexities in sentence-level language modeling, far below those of traditional n-gram models.",0.0,0.22459639126305794
256,one important work is proposed by who use wikipedia articles to build a bipartite graph and apply spectral clustering over it to discover relevant clusters .,one such work is proposed by scaiella et al which uses wikipedia articles to develop a bipartite graph and employs spectral clustering over it to discover relevant clusters .,a significant work is proposed by who use wikipedia articles to build a bipartite graph and apply spectral clustering over it to find relevant clusters.,0.3601405504290042,0.6914198606271776
257,"math word problems form a natural abstraction to a range of quantitative reasoning problems , such as understanding financial news , sports .",math word problems form a natural abstraction to a lot of these quantitative reasoning problems .,"math word problems form a natural abstraction to a range of quantitative reasoning problems such as understanding financial news, sports.",0.4423193362467974,0.8335382345709367
258,cite-p-19-5-7 proposed a supervised method to learn term embeddings for hypernymy .,cite-p-19-5-7 proposed a dynamic distance-margin model to learn term embeddings that capture properties of hypernymy .,cite-p-19-5-7 proposed a supervised method to learn term embeddings for hypernymy.,0.442444266172892,0.5662393162393162
259,this measure has been shown to correlate well with human judgements .,it was shown to correlate significantly with human judgments and behave similarly to bleu .,this behavioural measurement has been demonstrated to entail correlation with human judgements.,0.0,0.3453453453453454
260,"finally , the string regeneration problem can be viewed as a constraint satisfaction approach .",the string regeneration can also be viewed as a natural language realization problem .,"despite this, we see a constraint satisfaction approach in the role of string regeneration.",0.0,0.30027386541471046
261,"in this paper , we explore a “ cluster and label ” strategy to reduce the human annotation effort needed to generate subjectivity .",we show that a “ cluster and label ” strategy relying on these two proposed components generates training data of good purity .,"in this paper, we examine the method of reducing the human annotation effort in order to generate subjectivity in the form of a “ cluster and label ” tool.",0.16352670859125373,0.3939075630252101
262,contractor et al used an mt model as well but the focus of their work is to utilize an unsupervised method to clean noisy text .,"contractor et al , 2010 , used an mt model as well but the focus of his work is to generate an unsupervised method to clean noisy text in this domain .",contractor et al investigated the use of an unsupervised method for a clean text.,0.0,0.3373670700403374
263,"lord et al , 2015b ) analyzed the language style synchrony between therapist and client during mi encounters .",lord et al analyzed the language style synchrony between counselors and clients .,lord et al described the language style synchrony between therapist and client during mi encounters as a challenge to define their behavior.,0.21800193956058223,0.7668240850059032
264,"in this work , we further propose a word embedding based model that consider the word formation of ugcs to improve the prediction .","in this paper , we propose a weakly supervised learning framework to mine fine-grained and multiple-typed relations from chinese ugcs .",the results of this work include a linear term embedding model that addresses the concept of word formation as well as a multi-dimensional vector vector system.,0.0,0.0925925925925926
265,kim and hovy and bethard et al explore the usefulness of semantic roles provided by framenet for both opinion holder and opinion target extraction .,bethard et al and kim and hovy explore the usefulness of semantic roles provided by framenet for both opinion holder and opinion target extraction .,kim and hovy and bethard et al explore the usefulness of semantic roles provided by framenet for both opinion holder and opinion target extraction.,0.8266188889979827,0.993088
266,in this paper we developed an algorithm that uses global optimization to learn widely-applicable entailment rules between typed predicates .,in this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates .,"we also use a single method, built to improve the knowledge of predicates as predicates.",0.0,0.19035218253968253
267,this work proposes a framework better-suited to scaling belief tracking models for deployment in real-world dialogue systems operating over sophisticated application domains .,"in this paper , we have proposed a novel neural belief tracking ( nbt ) framework designed to overcome current obstacles to deploying dialogue systems in real-world dialogue domains .","in this paper, we propose a framework that can be deployed in real-world dialogue systems.",0.15329843583118236,0.45731707317073167
268,we then use extended lexrank algorithm to rank the sentences .,we then use an extension of the lexrank algorithm to rank sentences .,we use the lexrank algorithm to rank a sentence.,0.32728743199186994,0.6775541946145621
269,"domain labels , such as medicine , architecture and sport provide a natural way to establish semantic relations among word senses , which can be profitably used during the disambiguation process .","the hypothesis is that domain labels ( such as medicine , architecture and sport ) provide a natural and powerful way to establish semantic relations among word senses , which can be profitably used during the disambiguation process .","domain labels, such as medicine, architecture and sport, provide a natural way to establish semantic relations between word senses, which can be profitably used during the disambiguation process.",0.5862103212350185,0.7707031250000002
270,"to begin , all state sets are initialized to empty and the initial state math-w-2-3-9-140 is put into .","to begin , all state sets are initialized to empty and the initial state math-w-2-3-9-140 is put into so ; here _1_is the end-of-input marker .","to begin, all state sets are initialized to empty and the initial state math-w-2-3-9-140 is put into the initial state math-w-2-3-9-140.",0.6847516225198526,0.7615272373540856
271,"input format specifications are almost always described in natural languages , with these specifications .",it is standard practice to write english language specifications for input formats .,"input format specifications are most often described in natural languages, with them standardizations.",0.0,0.3230218855218855
272,"recently , zeng et al attempt to connect neural networks with distant supervision following the expressed-at-least-once assumption .","particularly , zeng et al proposed a piecewise convolutional neural network architecture , which can build an extractor based on distant supervision .","recently, zeng et al attempt to connect neural networks with distant supervision following the expressed-at-least-once assumption.",0.13014409578548178,0.3657064471879287
273,popescu and etzioni proposed a relaxation labeling approach to utilize linguistic rules for opinion polarity detection .,popescu and etzioni proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection .,popescu and etzioni developed an appropriate treatment for soundness evaluation.,0.0,0.2719512195121952
274,the fisher kernel for structured classification is a trivial generalization of one of the best known data-defined kernels for binary classification .,the fisher kernel is one of the best known kernels belonging to the class of probability model based kernels .,the fisher kernel for structured classification is a common type of data-defined kernel of binary classification.,0.0,0.20304568527918782
275,feature weights are tuned using pairwise ranking optimization on the mt04 benchmark .,the feature weights are tuned using pairwise ranking optimization .,the feature weights are trained on the mt4 benchmark in order to optimize performance.,0.17395797375642236,0.5451895043731778
276,"this task usually requires aspect-related text segmentation , followed by prediction or summarization .","this task usually requires aspect segmentation , followed by prediction or summarization .",we employ text segmentation as the task of profiling a text in a single column or a text ( cite-p-3-3-3 ).,0.0,0.14388489208633096
277,"latent semantic analysis ( lsa ) is a mathematical technique used in natural language processing for finding complex and hidden relations of meaning among words and the various contexts in which they are found ( landauer and dumais , 1997 ; landauer et al , 1998 ) .",latent semantic analysis ( lsa ) is a familiar technique that employs a word-by-document vsm ( cite-p-11-1-5 ) .,"latent semantic analysis ( lsa ) is a research and research paper examining natural language systems for identification and semantic knowledge of human relations between languages of text, ie, dj, and others",0.19121616355935114,0.1941747572815534
278,neubig et al present a discriminative parser using the derivations of tree structures as underlying variables from word alignment with the parallel corpus .,neubig et al present a bottom-up method for inducing a preorder for smt by training a discriminative model to minimize the loss function on the hand-aligned corpus .,neubig et al present a discriminative parser using the derivations of tree structures as underlying variables from word alignment with the parallel corpus.,0.16407709122771408,0.3916314412238324
279,we run parallel fda5 smt experiments using moses in all language pairs in wmt14 and obtain smt performance close to the top constrained moses systems training using all of the training material .,we run parfda smt experiments using moses in all language pairs in wmt15 and obtain smt performance close to the top constrained moses systems .,"for the nlp dataset, we developed a 2-gram model using the cmlm toolkit, which is trained in the fda5 field.",0.0,0.10080645161290322
280,cook and stevenson extend this work to create an unsuper-vised noisy channel approach using probabilistic models for common abbreviation types and choosing the english word with the highest probability after combining the models .,this work was then extended by to create an unsupervised noisy channel approach using probabilistic models for common abbreviation types and choosing the english word with the highest probability after combining the models .,cook and stevenson used the same structure to generate a noisy channel model using the probabilistic models.,0.0,0.24653096622793588
281,"by our method , the reordering problem is converted into a sequence labeling problem .","our model converts the decoding order problem into a sequence labeling problem , i.e . a tagging task .","by our method, the reordering problem is converted into a sequence labeling problem.",0.19610968603463957,0.42944103794543675
282,"we gathered training examples from parallel corpora , semcor , and the dso corpus .","finally , we made use of examples from the dso corpus and semcor as part of our training data .","we summarized the u.s.-based trainings on the parallel corpus, semcor, and the dso corpus.",0.0,0.3493764492072445
283,"on sentences of length 40 , our system achieves an f-score of 89 . 0 % , a 36 % relative reduction in error .","on sentences of length 40 , our system achieves an f-score of 89.0 % , a 36 % relative reduction in error over a generative baseline .","on sentences of length 40, our system achieves a f-score of 89.0 %, a 36 % relative reduction in error.",0.7151151476745128,0.8569865116477394
284,bleu is essentially a precision-based metric and is currently the standard metric for automatic evaluation of mt performance .,"bleu is a widely accepted baseline measure of mt quality at the system level and , as such , is an obvious choice for a baseline adequacy metric .",bleu is a precise metric that improves the classification of mt data by optimizing a single metric.,0.0,0.24669233151909375
285,"on a collection of 1 . 5 million documents and 423 queries , our method was found to lead to an improvement of 28 % in map and 50 % in p @ 5 , as compared to the state-of-the-art method .","on a data set composed of 1.5 million citations extracted with pubmed , our best model obtains an increase of 28 % for map and nearly 50 % for p @ 5 over the classical language modeling approach .","on a collection of 1.5 million documents and 423 queries, our method was found to lead to an improvement of 28 % in map and 50 % in p @ 5, as compared to the state-",0.0,0.4713688946015424
286,"it is used to support semantic analyses in the english hpsg grammar erg , but also in other grammar formalisms like lfg .","it is used to support semantic analyses in hpsg english grammar -erg , but also in other grammar formalisms like lfg .","here we use the language model to organize semantic analysis in an english hpsg grammar erg, but also in other grammar formalisms like lfg.",0.38719034678517417,0.7193833415719229
287,"overall , this is a difficult task even for human translators .","overall , there is a large body of work in the linguistics literature that argues that paraphrases are not restricted to strict synonymy .","overall, this is a difficult task even for human translators.",0.0,0.19561403508771935
288,"source and target words are at the two ends of a long information processing procedure , mediated by hidden states .","however , as illustrated in figure 1 , source and target word embeddings are at the two ends of a long information processing procedure .","source and target words are at the two ends of a long information processing procedure, mediated by hidden states.",0.4726679947070175,0.6865558274959913
289,uszkoreit et al describe a large-scale parallel document mining method that involves translating all source documents into english then using n-gram matching through multiple scoring steps .,"for large scale document level alignment , uszkoreit et al , proposed a distributed system that reliably mines parallel text from large corpora .",uszkoreit et al use n-gram analysis to identify gaps between the two languages.,0.0,0.16304347826086957
290,"in this paper , we present a novel approach to lexical selection where the target words are associated with the entire source sentence ( global ) .","in this paper , we present the details of training a global lexical selection model using classification techniques and sentence reconstruction models using permutation automata .","in this paper, we present a novel approach to lexical selection where the target words are associated with the entire source sentence ( global ) ( re, 2 ).",0.1814726708191192,0.40763891978466876
291,several product feature extraction techniques have been proposed in the literatures .,"several methods have been proposed , mainly in the context of product review mining .",several product feature extraction techniques have been proposed in the literatures.,0.0,0.4777848639455782
292,we quantitatively evaluate the use of open ie output against other dominant structures .,"we studied open ie¡¯s output compared with other dominant structures , highlighting their main differences .",we evaluate the quantitative value of open ie output against other dominant structures.,0.0,0.362309480754327
293,"this function is a convolution kernel , which is proven to be positive definite .",this zero-extension is known to preserve positive definiteness .,"here, we demonstrate that a convolution kernel is positive definite.",0.0,0.40322580645161293
294,"these representations can be used as features or inputs , which are widely employed in information retrieval , document classification and other nlp tasks .","such methods are highly scalable and have been applied in information retrieval , large-scale taxonomy induction , and knowledge acquisition .","this representation is an implicit representation which is known for document classification, information retrieval, and others.",0.0,0.24572649572649571
295,articles from current week are clustered separately in currently 5 languages .,articles from current week are clustered monolingually several times a day .,articles from the current week are clustered separately in currently 5 languages.,0.23901088824528133,0.555742958340361
296,"using our approach yields better accuracy than two baselines , a majority class baseline and a more difficult baseline of lexical n-gram features .",our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams .,"our approach yields better accuracy than two baselines, one majority class baseline and a more difficult baseline of lexical n-gram features.",0.17470942957770763,0.5593188572652549
297,"a similar method is presented in where wordnet synonyms , antonyms , and glosses are used to iteratively expand a list of seeds .","a similar method is presented in andreevskaia and bergler , where wordnet synonyms , antonyms , and glosses are used to iteratively expand a list of seeds .",an unigram can be used to iterate the phrase name of all the grammatical symbols in a dictionary.,0.0,0.2591674354243542
298,this paper explores the utilization of personalization features for the post-processing of recognition .,"in this paper , we explore the use of personalization in the context of voice searches rather than web queries .",this paper explores the use of customization features for the post-processing of recognition.,0.0,0.4053396582132215
299,"we present the tweetingjay system for detecting paraphrases in tweets , with which we participated in task 1 of semeval 2015 .","we described tweetingjay , a supervised model for detecting twitter paraphrases with which we participated in task 1 of semeval 2015 .",we present a semi-automated tweetingjay system for using a non-sensema-naive network of images collected in the public domain.,0.0,0.16129032258064516
300,summaries show the effectiveness of the proposed methods .,evaluation results demonstrate the effectiveness of the proposed methods .,summaries show the effectiveness of the proposed methods.,0.6496350258549114,0.7060399917542773
301,"in this setting , where we use both word-level and character-level representations , it is beneficial to use a smaller lstm than in the character-level only setting .","9 we found that in this setting , where we use both word-level and character-level representations , it is beneficial to use a smaller lstm than in the character-level only setting .",this setting is useful to use an implicit word-level representation.,0.0,0.19530120979396345
302,the underlying model used is a long shortterm memory recurrent neural network in a bidirectional configuration .,the proposed method is based on a deep learning architecture named long short term memory .,the underlying model is a short term memory recurrent neural network with high-speed multidirectional functions.,0.0,0.35778061224489793
303,in this paper we present the machine learning system submitted to the conll shared task 2009 .,"in this paper we present the crotal semantic role labelling system , which has been used in the conll 2009 shared task 1 .",in this paper we present the machine learning system submitted to the conll shared task 2009.,0.24633803357645362,0.514386570840846
304,"amr parsing is a new research problem , with only a few papers published to date ( flanigan et al. , 2014 ; wang et al. , 2015 ) and a publicly available corpus of more than 10,000 english/amr pairs .","amr parsing is the task of taking a sentence as input and producing as output an abstract meaning representation ( amr ) that is a rooted , directed , edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence ( banarescu et al. , 2013 ) .",amr parsing is a common problem for many people in english.,0.0,0.052083333333333336
305,"following li et al , we build the coupled sequence labeling model based on a bigram linearchain crf .","as shown in similar to the first step , we use a sequence labelling approach with a crf model .","following li et al, we train a simple coupled sequence labeling model using the bigram linearchain crf.",0.0,0.3738693467336683
306,this paper presents a novel unsupervised method for discovering intra-sentence level discourse relations .,this paper focused on unsupervised discovery of intra-sentence discourse relations for sentence level polarity classification .,"in this paper, we examine how to investigate relational networks between entities at a particular domain.",0.0,0.19483024691358025
307,"recently , many accurate statistical parsers have been proposed for english , for japanese ) .","in recent years , many accurate phrase-structure parsers have been developed , .","recent, some statistical parsers have been proposed for english, for japanese ).",0.0,0.4336734693877551
308,we explore the use of transductive semi-supervised methods .,"to tackle this problem , we propose algorithms for transductive semi-supervised learning .",we investigate our approach to transducing and semisupervised pre-trained communication approaches.,0.0,0.11627906976744186
309,riedel et al used knowledge bases in conjunction with surface patterns to learn custom representations for relation extraction .,"similarly , riedel et al learn universal schemas by matrix factorization without pre-defined relations .",riedel et al used the knowledge base to learn custom representations for relation extraction.,0.0,0.3407407407407408
310,"the most related to this study is the work of continuous space n-gram translation models , where the feed-forward neural network language model is extended to represent translation probabilities .","the research that comes closest to ours is the work of schwenk et al on continuous space ngram models , where a neural network is employed to smooth translation probabilities .",this study is a comparatively high degree of cross-cultural n-gram translation research.,0.0,0.10273972602739725
311,to incorporate the document-level information and the sentenceto-document relationship into the graph-based ranking process .,the document-level information and the sentenceto-document relationship are incorporated into the graph-based ranking algorithm .,to integrate document-level information and sentence to document relationship into the graph-based ranking process.,0.25336549464486463,0.5736625514403292
312,and we hope that it will serve as a guide for future research in the field .,we hope that these findings can serve as a guide for future research in the field .,the use of this information will guide future research in the field.,0.2907729016657212,0.4167691172854684
313,"suitable for neural machine translation , we propose to learn a decoding algorithm with an arbitrary decoding objective .","in this paper , we solely focus on the problem of decoding given a trained neural machine translation model .",the nbc model requires that the decoding process was selected by a single parameter.,0.0,0.12820512820512822
314,"the conll 2008 shared task was joint dependency parsing and srl , but the top performing systems decoupled the tasks , rather than building joint models .","the conll 2008 shared task was intended to be about joint dependency parsing and semantic role labeling , but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly .","conll shared task was joint dependency parsing and srl, but the top performing systems decoupled the tasks, instead of building joint models.",0.2982233752865516,0.3845092765018277
315,we take a more restrictive approach by additionally penalizing sequences similar to the out-domain data .,our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data .,our results show that we take a more restrictive approach by additionally penalizing sequences similar to the out-domain data.,0.3942058093215873,0.7244363714072015
316,in this paper suggest that current te systems may be able to provide open-domain q / a systems with the forms of semantic inference needed to perform accurate answer .,"in this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( q/a ) systems .",in this paper we propose that current te systems may be able to provide open domain q/a systems with the forms of semantic inference needed to perform accurate answer.,0.0,0.31816911250873514
317,"we compare with convolutional neural networks , recurrent neural networks , bidirectional gated recurrent neural networks , and word embeddings .","examples of such neural networks are linear networks , deeper feed-forward neural networks , or recurrent neural networks .","we compared to convolutional neural networks, recurrent neural networks, bidirectional gated recurrent neural networks, and word embeddings.",0.0,0.43151041666666673
318,the system by modeling the inference as an ilp problem with the features of narratives adopted as soft constraints .,the idea is to perform inference via a linear programming formulation with the features of narratives adopted as soft constraints .,"by modeling the inference as an ilp problem with the features of narratives adopted as soft constraints, et al, 2003.",0.3711335186021823,0.5957630903204197
319,domestic abuse is a problem of pandemic proportions ; nearly 25 % of females and 7.6 % of males have been raped or physically assaulted by an intimate partner ( cite-p-15-1-11 ) .,"domestic abuse is the 12 th leading cause of years of life lost ( cite-p-17-1-15 ) , and it contributes to health issues including frequent headaches , chronic pain , difficulty sleeping , anxiety , and depression ( cite-p-17-1-1 ) .",domestic abuse is a serious issue when it comes to discrimination and violence.,0.0,0.16481723237597912
320,no induces the f relation ; mutating cat to carnivore induces the math-w-4-2-0-46 relation .,"for example , in figure 1 , mutating the to no induces the f relation ; mutating cat to carnivore induces the math-w-4-2-0-46 relation .",mutating cat to carnivore induces the math-w-4-2-0-46 relation.,0.3441537868654125,0.38435158805529174
321,"in at least 95 % of cases , so we applied errant to the system output of the conll-2014 shared task to carry out a detailed error type analysis .",we demonstrated the value of errant by carrying out a detailed evaluation of system error type performance for all teams in the conll2014 shared task on grammatical error correction .,we present a detailed error type analysis using an errant method to compare conll 2014 data in a statistical analysis of both entities.,0.0,0.1530612244897959
322,we use an implementation of a maximum-entropy classifier called wapiti 8 .,"we employ the crf implementation in the wapiti toolkit , using default settings .","for this purpose, we applied a system called wapiti 8 for word-only classifier.",0.0,0.14184397163120566
323,we use wordsim 353 as the original data set .,"to that end , we use the wordsim353 dataset .","for our standardized use, we used the wordim with its original version.",0.0,0.3576923076923077
324,while cite-p-15-3-10 only showed results on a narrow domain of cooking videos with a small set of predefined objects and actors .,"also , they showed results only on the narrow domain of cooking videos with a small set of predefined objects and actors .",while cite-p-15-3-10 showed only results on a narrow domain of cooking videos with a small set of predefined objects and actors.,0.5831609706482522,0.8221340526678681
325,the kit system uses an in-house phrase-based decoder to perform translation .,the kit translations are generated by an in-house phrase-based translations system .,"as an example, the translation system uses a phrase-based decoder.",0.0,0.35532407407407407
326,"we adopted the second release of the american national corpus frequency data 2 , which provides the number of occurrences of a word in the written and spoken anc .","we adopted the second release of the american national corpus frequency data 3 , which provides the number of occurrences of a word in the written and spoken anc .","we adopted the second release of the american national corpus frequency data 2, which provides the number of occurrences of a word in the written and spoken anc.",0.9095930632220222,0.966508125247721
327,"in this paper , we present and make publicly available 1 a new dataset for darknet active domains , which we call it ” darknet usage .","in this paper , we present and make publicly available 1 a new dataset for darknet active domains , which we call it ” darknet usage text addresses ” ( duta ) .","in this paper, we propose and make publicly available 1 a new dataset for darknet active domains, which we call it “ darknet usage.",0.6198062800175154,0.7700246913580246
328,"as a language model feature , we use a standard backing off word-based trigram language model .",we integrate the recurrent neural network language model as an additional feature into the standard log-linear framework of translation .,"for language models, we use the tmln toolkit to support trigram languages.",0.0,0.19175257731958764
329,we evaluate our models using the standard bleu metric 2 on the detokenized translations of the test set .,"we evaluated the models using the wmt data set , computing the ter and bleu scores on the decoded output .","for the detokenization of our formulas, we used the bleu metric 2 to evaluate our translation models.",0.0,0.21634615384615385
330,we used cohen 魏 to measure the inter-annotator agreement .,we also computed the inter-annotator agreement via kappa .,"for a total of 690 languages, we used cohen as the interannotator agreement.",0.0,0.20833333333333331
331,"on the standard parseval metric matches that of the ( cite-p-16-3-5 ) parser on which it is based , despite the data fragmentation caused by the greatly enriched space of possible node labels .","the accuracy of the first-stage parser on the standard parseval metric matches that of the ( cite-p-16-3-5 ) parser on which it is based , despite the data fragmentation caused by the greatly enriched space of possible node labels .","on the standard parseval metric matches that of the ( cite-p-16-3-5 ) parser on which it is based, despite the data fragmentation caused by the greatly enriched space of possible no",0.7775028408288042,0.7908030537917561
332,statistical part implements an entropy based decision tree ( c4 . 5 ) .,the statistical part implements an entropy based decision tree ( c4.5 ) .,the statistical part is a model of a c4. 5 entropy-based decision tree.,0.0,0.42613636363636365
333,"by deeply integrating semantic frame criteria into the mt training pipeline , it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame .","however , to go beyond tuning weights in the loglinear smt model , a cross-lingual objective function that can deeply integrate semantic frame criteria into the mt training pipeline is needed .","by integrating semantic frame criteria into the mt training pipeline, it is necessary to have a cross-lingual semantic objective function that evaluates the semantic frame in our training training.",0.2966102585621941,0.4947100313479623
334,"tang et al design preference matrices for each user and each product to tune word representations , based on which convolutional neural networks are used to model the whole document .","tang et al design user and product preference matrices to tune word representations , based on which convolutional neural networks are used to model the whole document .","tang et al create preference matrices for each user and each product for each word representation, based on which convolutional neural networks are used to model the entire document.",0.4686633676296947,0.8414310954063603
335,"for our first hypothesis , we induce pos distribution information from a corpus , and approximate the probability of occurrence of pos blocks .","as a first step , to test our first hypothesis , we remove the pos blocks with a low probability of occurrence from each query , on the assumption that these blocks are content-poor .","we first assume the probability of occurrence of the posed blocks based on a group of resources, so that they can be converted into a single pg.",0.0,0.20348837209302328
336,"in this paper , we investigate discriminative reranking upon a baseline semantic parser .","in this paper , we investigate the effect of discriminative reranking to semantic parsing .","in this paper, we investigate discriminative reranking upon a baseline semantic parser.",0.38739209989720524,0.6496644295302012
337,"when combined with word deviations and mention percentages , most persuasive argumentation features give superior performance compared to the baselines .","when combining with content-related features , most persuasive argumentation features give superior performance compared to the baselines .","in this spiral representation, language development results in high standard performance.",0.0,0.08571428571428572
338,"hyp consists of a c + + api , as well as a command line tool , and is available for download .",the hyp toolkit provides a c++ library and a command line executable .,"hyp consists of a c + + api as well as a command line tool, and is available for download.",0.1709686260975486,0.4118337982675084
339,"firstly , for computing the lexical and string similarity between two sentences , we take advantage from the task baseline which is a system using a logistic regression model with eighteen features based on n-grams .",we use the system described in the literature to compute the lexical and string similarity between two sentences by using a logistic regression model with eighteen features based on n-grams .,the task baseline is a simple logistic regression model with eighteen features based on n-grams.,0.23888078162891777,0.40360169491525416
340,coreference resolution has traditionally benefited from machine learning approaches .,some machine learning approaches have been applied to coreference resolution .,"since snider-coreference resolution, a recurrent clustering technique has benefited from a range of neural networks.",0.0,0.08620689655172414
341,the language model is a 5-gram lm with modified kneser-ney smoothing .,it is based on 5-grams with extended kneser-ney smoothing .,the language model is a 5-gram language model with modified kneser-ney smoothing.,0.0,0.4962243797195253
342,"we present a pro , a new method for machine translation tuning .","we have presented a pro , a new tuning method for machine translation .","we present a pro, a new method for machine translation tuning.",0.4820099990656715,0.9086458643735899
343,experimental results on real-world datasets show that our model achieves significant and consistent improvements on relation extraction .,the experimental results show that our model achieves significant and consistent improvements as compared with baselines .,experimental results on real-world datasets show that our model achieves significant and consistent improvements on relation extraction.,0.49852287898736825,0.6962719298245614
344,"in an enc ¨c dec model , a long input sequence results in performance degradation due to loss of information in the front portion of the input sequence .",the rnn encoder¨cdecoder model suffers from poor performance when the length of the input sequence is long .,"in an enc c dec model, a long input sequence results in performance degradation due to loss of information in the front portion of the input sequence.",0.09865637517691195,0.4013961605584643
345,"in our work , we develop our active dual supervision framework using constrained non-negative tri-factorization .",our work is based on the dual supervision framework using constrained non-negative tri-factorization proposed in ( cite-p-17-1-10 ) .,in order to accomplish this we employ the constrained non-negative trifactorization of the subsystem.,0.0,0.22924133811230585
346,use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains .,"however , large parallel corpora are only available for a few language pairs and for limited domains .",the use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains.,0.5674773954614979,0.8464575066137564
347,"in this paper , we investigate the difference between word and sense similarity measures .","thus , in this paper we investigate whether similarity should be measured on the sense level .","in this paper, we examine word similarity measures.",0.0,0.38726993865030673
348,some researchers have found that transliteration is quite useful in proper name translation .,some researchers have applied the rule of transliteration to automatically translate proper names .,"to date, some researchers have demonstrated that transliteration is quite useful in proper name translation.",0.0,0.5361305361305361
349,"morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text , according to the word context .",morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .,"the morphological disambiguation technique involves assigning one set of morphological features to each individual word in a text, according to the word context.",0.5319499812335498,0.786057895181028
350,mikolov et al introduced a particularly simple version that takes advantage of a vocabulary of shared bilingual seed words to map embeddings from a source language onto the vector space of a target language .,mikolov et al extended this model to two languages by introducing bilingual embeddings where word embeddings for two languages are simultaneously represented in the same vector space .,mikolov et al introduced a simple version that took advantage of a vocabulary of shared bilingual seed words to map embeddings from a source language onto the vector space of a target language.,0.0,0.37078992436135294
351,"this paper proposes a novel japanese pas analysis model based on a neural network ( nn ) framework , which has been proved to be effective for several nlp tasks .",this paper presents a novel model for japanese predicate argument structure ( pas ) analysis based on a neural network framework .,"we present a new model for japanese pas based on neural network ( nn ), used to test our accuracy and performance.",0.0,0.5468606361463504
352,"we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs , which is computationally feasible .","therefore , we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs .","in this paper, we propose a divide-and-conquer strategy, by decomposing a hypergraph into a set of independent subhypergraphs, which is computationally feasible.",0.5311613208274918,0.8983656040639034
353,bunescu and mooney connect weak supervision with multi-instance learning and extend their relational extraction kernel to this context .,"bunescu and mooney , 2007 ) connects weak supervision with multi-instance learning and extends it to relation extraction .",bunescu and mooney present a relational extraction kernel with multi-instance learning.,0.0,0.418943533697632
354,v-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness .,v-measure assesses the quality of a clustering solution against reference clusters in terms of clustering homogeneity and completeness .,v-measure evaluates the quality of a clustering solution by explicitly measuring its homogeneity and its completeness.,0.3170497934957707,0.4725690170564446
355,liu et al proposed two models capturing the interdependencies between the two parallel lstms encoding two input sentences for the tasks of recognising textual entailment and matching questions and answers .,"liu et al propose two models that capture the interdependencies between two parallel lstms encoding the two sentences for the tasks of recognizing textual entailment and matching questions and answers , respectively .",liu et al proposed two models capturing the interdependencies between the two parallel lstms encoding two input sentences for the task of recognising textual entailment and matching questions,0.3013673575243289,0.78340919435594
356,but they produce differing characterizations of non-arbitrariness .,"however , they attribute responsibility for non-arbitrariness differently .","in all telecommunications products, the nlp methods are highly regarded as incompatible with the interpolated domain.",0.0,0.10101010101010102
357,the former approach involves adding self-labeled data from the target domain produced by a model trained in-domain .,the former approach involves adding self-labelled data from the target domain produced by a model trained in-domain .,"in the last 4 months, we will use the non-coding model and new language-language training ( entails an unlabeled data, in terms of a specific keyword, or the default, of",0.0,0.1530612244897959
358,temporal and causal relations are closely related .,the temporal relation is dictated by the causal relation .,the temporal and causal relations in the literature are closely related with.,0.0,0.46562314246086783
359,"moreover , some systems and , also exploit kinds of extra information such as the unlabeled data or other knowledge .","moreover , some systems and zhang et al , also exploit kinds of extra information such as unlabeled data or other knowledge .",we also exploit various types of information.,0.0,0.17302325581395347
360,"argumentation features derived from a coarse-grained , argumentative structure of essays are helpful in predicting essays .",we investigate whether argumentation features derived from a coarse-grained argumentative structure of essays can help predict essays scores .,"arguments based on a coarse-grained structure of essays, argueative argumentation features, can be useful in predicting essays.",0.0,0.565917684700407
361,"sentence similarity computation plays an important role in text summarization , classification , question answering and social network applications .",sentence similarity computation plays an important role in text summarization and social network applications .,"sentence similarity computation plays an important role in text summarization, classification, question answering and social network applications.",0.6383240325919926,0.966594982078853
362,the power prediction system is built using the cleartk wrapper for svmlight package .,the taxonomy kernel was trained using the svm 脨 脴 package .,"the word embeddings is trained on the svmlight corpus, with the cleartk wrapper.",0.0,0.16260162601626016
363,"in english , we have demonstrated a 33 % relative reduction in error rate .","with this approach , we reduce the error rate for english by 33 % , relative to the best existing system .",we show that the standard deviations of errors in text were 33 % the same.,0.0,0.2486127336448598
364,we use marginal inference in a conditional random field .,"in particular , we consider conditional random fields and a variation of autoslog .",we use the marginal inference method in order to use the general public inference.,0.0,0.10638297872340424
365,we use a random forest classifier consisting of a combination of decision trees where features are randomly extracted to build each decision tree .,"we employ a random forest classifier , an ensemble of decision tree classifiers learned from many independent subsamples of the training data .",this classifier can be used as a random forest classifier.,0.08333419026210014,0.291143980527991
366,"indicating the importance of each fact or pair of facts , they select the facts to express .",they usually start by selecting the logical facts to express .,"indicating the importance of each fact or pair of facts, they select the facts to express.",0.1487964117124549,0.5747427175998604
367,"since chinese is the dominant language in our data set , a word-by-word statistical machine translation strategy ( cite-p-14-1-22 ) is adopted to translate english words into chinese .",this is because chinese is a pro-drop language ( cite-p-21-3-1 ) that allows the subject to be dropped in more contexts than english does .,"with chinese being the main language, the chinese word model is adopted in the process of translating the english word into chinese.",0.0,0.1606425702811245
368,"from the second experiment , we can conclude that taking definition structure into account helps to get better classification .",in the second experiment we show that classification results improve when information on definition structure is included .,"from the second experiment, we can conclude that taking definition structure into account helps to get better classification.",0.0,0.42124542124542125
369,"messages on microblogs are short , noisy , and informal texts with little context , and often contain phrases with ambiguous meanings .","the reason is that messages on microblogs are short , noisy and informal texts with little context , and often contain phrases with ambiguous meanings .","microblogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings.",0.740818220681718,0.7873523622047245
370,our baseline system is an standard phrase-based smt system built with moses .,our baseline system was a vanilla phrase-based system built with moses using default settings .,"our baseline system is a meta-based phrase-based smt system, built with moses.",0.0,0.5986577181208053
371,"in prototype-driven learning , we specify prototypical examples .",we investigate prototype-driven learning for primarily unsupervised sequence modeling .,"in prototype-driven learning, we construct a prototypical word-of-mouth approach with an optional annotation.",0.0,0.3005952380952382
372,we use the berkeley parser to parse all of the data .,"for comparison , we also include the berkeley parser .","we use berkeley, which enables us to parse all of our data.",0.0,0.19230769230769232
373,"in this paper , we presented allvec , an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples .","in this work , we propose allvec that uses batch gradient learning to generate word representations from all training samples .",we also used allvec as an experimental batch learning tool that can be used for syllable and negative training.,0.0,0.21531100478468898
374,"that with proper representation , large number of deterministic constraints can be learned from training examples .","we have shown by experiments that large number of deterministic constraints can be learned from training examples , as long as the proper representation is used .","we also propose that with proper representation, large number of deterministic constraints can be learned from training examples.",0.411068502879962,0.632178615127554
375,"gao et al and moore and lewis apply this method to language modeling , and foster , goutte , and kuhn and axelrod , he , and gao apply this method to translation modeling .","gao et al and moore and lewis apply this method to language modeling , while foster et al and axelrod et al use it on the translation model .","gao et al, moore and lewis, sl., and pt, and gao apply this method to language modeling, and foster, goutte, and kuh",0.29166980826665706,0.484375
376,"relevant applications deal with numerous domains such as blogs , news stories , and product reviews .",relevant applications deal with numerous domains such as news stories and product reviews .,"applications deal with numerous domains, including blogs, news stories, and product reviews.",0.4169392927528885,0.7667908276102898
377,"we use a linear classifier trained with a regularized average perceptron update rule as implemented in snow , .","we use a linear classifier trained with a regularized perceptron update rule as implemented in snow , .","we use the snr training exercise, which is the fourth language to train an nlr classifier in a multi-level snow model.",0.0,0.31564898231564903
378,all classifiers and kernels have been implemented within the kernel-based learning platform .,all classifiers and kernels have been implemented within the kernel-based learning platform called kelp .,"the classifiers and kernels based on the kernel-based learning platform, have been implemented for the majority of training sessions.",0.21518838690610018,0.6720173765628311
379,our experiments show that r ealm reduces extraction error .,our experiments demonstrate that r ealm outperforms these approaches on sparse data .,we used a method known as r ealm to track the extraction error.,0.0,0.1950805767599661
380,"recent question answering systems have focused on open-domain factoid questions , relying on knowledge bases like freebase or large corpora of unstructured text .","in recent years , the development of largescale knowledge bases , such as freebase , provides a rich resource to answer open-domain questions .","recent question answering systems focus on open-domain factoid questions, relying on knowledge bases such as freebase, or large corpora of unstructured text.",0.1333497993210919,0.4499251147830784
381,"approaches to dependency parsing either generate such trees by considering all possible spanning trees , or build a single tree on the fly by means of shift-reduce parsing actions .","approaches to dependency parsing either generate such trees by considering all possible spanning trees , or build a single tree by means of shift-reduce parsing actions .",dependency parsing is a common approach when it comes to defining a dependency tree by considerationing all possible spanning trees.,0.12501012357600622,0.3872053872053872
382,"in this paper , we presented the methods we used while participating in the 2016 clinical tempeval task .","in this paper , we present the methods we used while participating in the 2016 clinical tempeval task as part of the semeval-2016 challenge .","in this paper, we present the methods we used to participate in the 2016 clinical tempeval task.",0.5428066659565502,0.7160114349321999
383,we provide an analysis of humans ¡¯ subjective perceptions of formality in four different genres .,we perform an analysis of humans¡¯ perceptions of formality in four different genres .,we have studied formality perceptions in four different genres using the human perception model.,0.195647514979229,0.49811613475177297
384,"in this paper , we propose a framework for automatically identifying reasons in online reviews .","in this paper , we present a system that automatically extracts the pros and cons from online reviews .",we use a framework for automatic identifying reasons in online reviews for a sample review of literature.,0.0,0.2537522945686211
385,we exploit the svm-light-tk toolkit for kernel computation .,we employ the ranking mode of the popular learning package svm light .,the kernel computation system is trained with the svm-light-tk toolkit.,0.0,0.11718749999999999
386,paper presents a step toward semantic grounding for complex problem-solving dialogues .,this paper presents a novel approach to semantic grounding of noun phrases within tutorial dialogue for computer programming .,paper presents an approach for the conceptual semantic grounding of complex problem-solving dialogues.,0.0,0.4144144144144144
387,refinement process continues until the two base rankers can not learn from each other any more .,this process continues until the two base rankers can not learn from each other any more .,this refinement process continues until the two base rankers can not learn from each other anymore from the lsp and e.,0.5795086255869999,0.8135873015873015
388,"choi et al address the task of extracting opinion entities and their relations , and incorporate syntactic features to their relation extraction model .",choi et al used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction .,choi et al discuss the issue of extracting opinion entities and their relations from their relation extraction model.,0.0,0.4029255319148936
389,pereira et al suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns .,"pereira et al use an information-theoretic based clustering approach , clustering nouns according to their distribution as direct objects among verbs .",pereira et al proposed deterministic annealing to cluster verb-argument pairs into classes of verb and nouns.,0.0,0.2936046511627907
390,"in addition , the average accuracy of the classifier is 81 . 5 % on the sentences .","in addition , the average accuracy of the classifier is 81.5 % on the sentences the judges tagged with certainty .",the grade of the classifier is 81. 5 % on the words and thus 81.5% is better than on the sentences.,0.1905195539361548,0.4445422535211268
391,"by extrinsic evaluation , i . e . , we applied the results of topic detection to extractive multi-document summarization .","in order to evaluate the method , we applied the results of topic detection to extractive multi-document summarization .",we use the results of extrinsic evaluation to extract a single file summarization.,0.0,0.4447781114447781
392,natural language generation is the process of generating coherent natural language text from non-linguistic data .,data-to-text generation refers to the task of automatically generating text from non-linguistic data .,natural language generation is the task of extracting a coherent natural language and its use in translating non-linguistic data to a full-fledged one in a category.,0.0,0.4590482026143791
393,"for the majority of tasks , we find that simple , unsupervised models perform better when n-gram frequencies are obtained from the web .","we showed that simple , unsupervised models using web counts can be devised for a variety of nlp tasks .","in the end, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web.",0.18185234853618293,0.407346290184574
394,"given a user ¡¯ s tweet sequence , we define the purchase stage identification task as automatically determining for each tweet .","in particular , we define the task of classifying the purchase stage of each tweet in a user¡¯s tweet sequence .","as a user  s tweet sequence, we define the purchase stage identification task as automatic determining for each tweet.",0.208795826063924,0.5163426317272471
395,"cohn et al , the annotators were instructed to distinguish between sure and possible alignments , depending on how certainly , in their opinion , two predicates describe the same event .","cohn et al , the annotators were instructed to distinguish between sure and possible alignments , depending on how certainly , in their opinion , two predicates describe verbalizations of the same event .","cohn et al used specific predicates to distinguish between any unigramal alignments, depending on the nature of the same word.",0.1466691691638711,0.44439548399390244
396,the combination of a heightened learning rate and greedy processing results in very reasonable one-shot learning .,it requires a high-risk strategy combining heightened learning rate and greedy processing of the context .,the combination of a heightened learning rate and greedy processing results in very reasonable one-shot learning.,0.32014060615821366,0.5518197217801961
397,"from a computational point of view , distinguishing between antonymy and synonymy is important for nlp .",distinguishing between antonyms and synonyms is a key task to achieve high performance in nlp systems .,the relationship between synonymy and antonymy is important for nlp.,0.0,0.15243902439024393
398,"on the same topic , most tdt approaches rely on traditional vector space models .",most existing approaches rely on the traditional vector space model .,"in addition, most tdt approaches rely on vector space models to improve image quality.",0.0,0.6521739130434784
399,a gaussian prior is used for regularizing the model .,we use the gaussian prior smoothing method for the language model .,a gaussian prior is used for regularizing the model.,0.0,0.485126253891387
400,current event extraction systems rely on local information .,several recent studies use high-level information to aid local event extraction systems .,the event extraction system uses local information to extract event data.,0.0,0.4893410852713178
401,li et al investigated the prediction of places of interest based on linear rank combination of content and temporal factors .,li et al rank a set of candidate points of interest using language and temporal models .,li et al presented the first research on a linear rank combination of content and temporal factors.,0.0,0.44834307992202727
402,brown clustering is a hierarchical clustering method that groups words into a binary tree of classes .,brown clustering is a commonly used unsupervised method for grouping words into a hierarchy of clusters .,brown clustering is a hierarchical clustering task that can be used to generate a binary tree of keywords.,0.15071676257541072,0.33950617283950624
403,dinu and lapata propose a probabilistic framework for representing word meaning and measuring similarity of words in context .,and dinu and lapata propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors .,dinu and lapata proposed a probabilistic framework for representing word meaning and measuring similarity of words in context.,0.0,0.4363479262672811
404,"we use the penn discourse treebank , which is the largest handannotated discourse relation corpus annotated on 2312 wall street journal articles .","the penn discourse treebank we use the penn discourse treebank , the largest available manually annotated corpora of discourse on top of one million word tokens from the wall street journal .","we use the penn discourse treebank, which is the largest handannotated discourse relation corpus annotated on 2312 wall street journal articles.",0.24146387319230583,0.45166800643086813
405,we introduce a novel method to aggregate the variable-cardinality boew into a fixed-length vector by using the fk .,"then , the variable size boews are aggregated into fixed-length vectors by using fk .",we introduce a method to aggregate the variable-cardinality boew into a fixed-length vector utilizing the fk.,0.0,0.4152960526315789
406,to transfer the semantic difference vector to a probability distribution over similarity scores .,5 ) transfer the semantic difference vector to the probability distribution over similarity scores by fully-connected neural network .,to transfer the semantic difference vector to a probability distribution over similarity scores.,0.44066285924968884,0.6251876876876877
407,to calculate the constituent-tree kernels st and sst we used the svm-light-tk toolkit .,we used the svm-light-tk 5 to train the reranker with a combination of tree kernels and feature vectors .,we used the svm-light-tk toolkit to track the kernel and tracer levels.,0.14495639555867468,0.4471953837895867
408,"maas et al present a probabilistic topic model that exploits sentiment supervision during training , leading to rep- resentations that include sentiment signals .","maas et al presented a probabilistic model that combined unsupervised and supervised techniques to learn word vectors , capturing semantic information as well as sentiment information .","maas et al, 2014 ) proposed a probabilistic topic model to employ sentiment oversight during training.",0.0,0.3174329501915709
409,"in order to evaluate the retrieval performance of the proposed model on text of cross languages , we use the europarl corpus 2 which is the collection of parallel texts in 11languages from the proceedings of the european parliament .","we evaluate the proposed triangulation method through pivot translation experiments on the europarl corpus , which is a multilingual corpus including 21 european languages widely used in pivot translation work .","in order to evaluate the retrieval performance of the proposed model on text of cross languages, we use the europarl corpus 2 which is the collection of parallel texts in 11 language texts from the proceedings of the european parliament.",0.0,0.2930904400311526
410,"in later work , this idea was extended to the disambiguation of translations in a bilingual dictionary .","in later work , this idea was applied to the disambiguation of translations in a bilingual dictionary .","furthermore, in the development of a bilingual dictionary, the idea was extended to the disambiguation of translations in a bilingual dictionary.",0.41872968686289325,0.7439104674127717
411,"in this paper , we have proposed a novel topic model for hypertexts .","in this paper , we study the problem of topic modeling for hypertexts .",we propose an unstructured hypertext-based model on the first-in-the-sequence model.,0.0,0.145985401459854
412,"zhang and clark proposed a wordbased cws approach using a discriminative perceptron learning algorithm , which allows word-level information to be added as features .","zhang and clark proposed a graphbased scoring model , with features based on complete words and word sequences .",zhang and clark used the discriminative perceptron tool to construct a word-based cws toolkit to get a user-defined look at word-level information in text.,0.0,0.12755102040816324
413,"the current release of the odin ( online database of interlinear text ) database contains over 150 , 000 linguistic examples , from nearly 1 , 500 languages , extracted from pdfs found on the web .","the current release of the odin ( online database of interlinear text ) database contains over 150,000 linguistic examples in the form of interlinear glossed text ( igt ) , an example of which is shown in fig . 1 .","the current release of the odin ( online database of interlinear text ) database contains over 150, 000 linguistic examples, from nearly 1, 500 languages, extracted from pdfs found on the web.",0.40242964125424296,0.4559958067011359
414,phrase structures and dependency structures are two of the most popular grammar formalisms for statistical parsing .,probabilistic context-free grammars are commonly used in parsing and grammar induction systems .,phrase structures and dependency structures are the most popular grammar formsalisms for statistical parsing.,0.0,0.1893939393939394
415,"chambers and jurafsky learn narrative schemas , which mean coherent sequences or sets of events , from unlabeled corpora .","for instance , chambers and jurafsky model narrative flow in the style of schankian scripts .","chambers and jurafsky and other jurafsky interpret narrative schemas, which mean coherent sequences or sets of events, from unlabeled corpora.",0.0,0.20958083832335328
416,"in all current deep compositional distributional settings , the word embeddings are internal parameters of the model .","in all the above models , the word embeddings and the weights of the compositional layers are optimized against a task-specific objective function .",most large scale word embeddings are internal parameters of the model in all current deep compositional distributional settings.,0.0,0.3525531914893617
417,zelenko et al used the kernel methods for extracting relations from text .,zelenko et al and culotta and sorensen used tree kernels for relation extraction .,zelenko et al used a method that extracts relations from text by using a kernel kernel.,0.0,0.44143356643356646
418,gao et al model interestingness between two documents with deep nns .,gao et al modeled interestingness between two documents with deep nns .,gao et al proposed a linear comparison to two paper documents with deep nns.,0.33887143631861766,0.7181571815718157
419,we describe our deep convolutional neural network for sentiment analysis of tweets .,this paper describes our deep learning system for sentiment analysis of tweets .,we describe the neural network for convolutional tweet analysis.,0.0,0.1968503937007874
420,we propose an inter-weighted layer to measure the importance of different parts .,"from this point , we propose a novel inter-weighted layer to measure the importance of each word .",we propose a cross-weighted layer to measure the importance of various parts.,0.3385568963210437,0.5637142857142857
421,input to the pos feature is obtained from the twitter partof-speech tagger .,the tweets are tokenized using the cmu pos tagger .,the translations are automatically assigned to the pos feature by the twitter partof-speech tagger.,0.0,0.40608465608465605
422,vuli膰 et al utilize the vsm to produce target clusters which are compared to the groupings from the lexical resource via collocation and purity .,vuli膰 et al and sun et al apply a clustering algorithm to the input words and measure how well the clusters correspond to the word groupings in verbnet via purity and collocation .,"vuli et al, 2006 ) used the vsm to produce target clusters compared to the groupings from the lexical resource via collocation and purity.",0.0,0.25991975737663486
423,"bilingual dictionaries , parallel corpora , machine translators , morphological analyzers , and so on ) on the internet , most intercultural collaboration activities are still lacking multilingual support .","although there are many language resources on the internet , most intercultural collaboration activities still lack multilingual support .",many intercultural collaboration activities have been found to be a major problem in the internet.,0.0,0.33940849066899487
424,"semantic relatedness is a very important factor for coreference resolution , as noun phrases used to refer to the same entity should have a certain semantic relation .",semantic relatedness is a very important factor for the coreference resolution task .,"semantic relatedness is a key aspect of focusing on semantic relatedness, since noun phrases used to refer to the same entity should have a certain semantic relation.",0.08914609827874222,0.20547945205479454
425,evaluation metrics we use the ribes and the bleu scores as evaluation metrics .,evaluation metrics we report bleu as the main evaluation metric of the question generation systems .,we use the ribes and bleu scores as evaluation metrics.,0.0,0.22580645161290322
426,"in our model , we use negative sampling discussed in to speed up the computation .",we use negative sampling to approximate softmax in the objective function .,"a similar model is done, using negative sampling to accelerate the computation.",0.0,0.46106557377049184
427,we used the pre-trained word embeddings that are learned using the word2vec toolkit on google news dataset .,"as for english , we used a pretrained google news word embeddings 2 , which has shown high performance in several word similarity tasks .",we used a free 2nd word embedding with pre-trained word2vec tools to train the learning model.,0.0,0.21120293847566574
428,distributional semantics builds on the assumption that the semantic similarity of words is strongly correlated to the overlap between their linguistic contexts .,distributional semantics is based on the hypothesis that words co-occurring in similar contexts tend to have similar meaning .,a similar semantic model builds on the assumption that the semantic similarity of words is strongly correlated with the overlap between their linguistic contexts.,0.0,0.2551020408163265
429,"this representation consists of distributional features , suffixes and word shapes of math-w-2-1-1-32 and its local neighbors .",this representation is the basis for the lexical-semantic level that is included in the kr component .,"this representation consists of distributional features, suffixes and word shapes of math-w-2-1-1-32 and its local neighbors.",0.0,0.14944769330734245
430,tag is a tree-rewriting system : the derivation process consists in applying operations to trees in order to obtain a ( derived ) tree whose sequence of leaves is a sentence .,"although tag is a class of tree rewriting systems , a derivation relation can be defined on strings in the following way .",tag is a highly effective language based implementation of the tag data.,0.0,0.13636363636363635
431,details about svm and krr can be found in .,details about svm and kfd can be found in .,further information about svm and krr can be found in.,0.46924700641055994,0.785891089108911
432,we used glove 10 to learn 300-dimensional word embeddings .,we used crfsuite and the glove word vector .,we use a training tool called glove 7.,0.0,0.3506944444444444
433,"in this paper , we propose an entity recognition system that improves this neural architecture .",this paper presents novel methods to improve neural entity recognition tasks .,"in this paper, we propose an entity recognition system, which improves this neural structure.",0.0,0.3836734693877551
434,"unfortunately , pinchak and lin use a brittle generative model when combining question contexts that assumes all contexts are equally important .","the pinchak and lin system is unable to assign individual weights to different question contexts , even though not all question contexts are equally important .",pinchak and lin used a brittle generative model to combine question contexts that assume all contexts are equally important.,0.2042573616508289,0.4429133858267717
435,the word ¡° granite ¡± is a pun with the target ¡° granted ¡± .,"here , ¡°clothed¡± is the pun and ¡°closed¡± is the target .",the word ° granite is a non-native term that uses the ° granted .,0.0,0.1639344262295082
436,"on the right hand side of the current word is not utilized , which is a relative weakness of shift-reduce parsing .","however , due to the incremental nature of shift-reduce parsing , the right-hand side constituents of the current word can not be used to guide the action at each step .","the wrong hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing.",0.11686713058269761,0.3020710059171598
437,lebret et al generate the first sentence of a biography using a conditional neural language model .,lebret et al generate the first sentence of a biography by a conditional neural language model .,lebret et al used the conditional neural language model to produce the first sentence of a biography.,0.4806604068305994,0.7864900346103353
438,"we used the first 200 movie reviews from the dataset provided by zaidan et al , with an equal distribution of positive and negative examples .",we use the movie reviews dataset from zaidan et al that was originally released by pang and lee .,we used a large sample of movie reviews from zaidan et al from sylvi.,0.16102642769112474,0.44238683127572015
439,"on the simlex999 word similarity dataset , our model achieves a spearman ’ s math-w-1-1-0-111 score of 0 . 517 , compared to 0 . 462 of the state-of-the-art word2vec model .","on simlex999 , our model is superior to six strong baselines , including the state-of-the-art word2vec skip-gram model by as much as 5.5–16.7 % in spearman ’ s ρ score .","on the simlex999 word similarity dataset, we achieve a spearman ’ s math-w-1-1-0-111 score of 0. 517, compared to 0. 462",0.0,0.2627501087429317
440,"our domains were taken from conceptual specifications in , which cluster semantically and encyclopedically related concepts to ensure a generally applicable set of domains involved in meaning shifts .","instead , our source and target domains were taken from specifications in , which we assumed to ensure a more stratified and generally applicable set of domains involved in meaning shifts .",we incorporated our domains into conceptual specification structures by forming semantically and encyclopedically related concepts.,0.0,0.09868421052631579
441,we segment the chinese half of the corpus using the maximum entropy segmenter from .,we use the maximum entropy segmenter of to segment the chinese part of the fbis corpus .,we segment the chinese half of the corpus using the maximum entropy segmenter.,0.24853582849586087,0.5426425255996882
442,"in particular , the recent shared tasks of conll 2008 tackled joint parsing of syntactic and semantic dependencies .","this is , in part , inspired by the recent conll shared task , which was the first evaluation of syntactic and semantic dependency parsing to include unmarkable nominals .","in particular, recent shared tasks of conll 2008 tackled joint parsing of syntactic and semantic dependencies.",0.0888941460369466,0.3987919132149901
443,in this paper shows that some kind of guiding technique has to be considered when one wants to increase parsing efficiency .,"in this paper , we study a parsing technique whose purpose is to improve the practical efficiency of rcl parsers .",we explore the benefits of using the parsing toolkit to improve the parsing efficiency.,0.0,0.30943627450980393
444,"between reviewers and products , we employ tensor decomposition to learn the embeddings of the reviewers and products in a vector space .","next , we utilize tensor factorization to perform tensor decomposition , and the representations of reviewers and products are embedded in a latent vector space by collective learning .",we utilize tensor decomposition for learning embeddings between reviewers and products in an vector space.,0.0,0.4146284151838164
445,"in the context of this discussion , we will refer to the target partitions , or clusters , as classes , referring only to hypothesized clusters .","in the context of this discussion , we will refer to the target partitions , or clusters , as classes , referring only to hypothesized clusters as clusters .","to this end, we would refer to clusters, or target partitions, as classes, referring only to hypothesized clusters.",0.3930487853888553,0.6602112676056338
446,"the penn discourse treebank is the largest available annotated corpora of discourse relations over 2,312 wall street journal articles .","the pdtb is the largest corpus annotated for discourse relations , formed by newspaper articles from the wall street journal .","this corpus consists of the penn discourse treebank, the largest available annotated corpus of discourse relations over 2,312 wall street journal articles.",0.0,0.47142817457010305
447,"however , most models of topic segmentation ignore the social aspect of conversations .","however , topic models alone can not model the dynamics of a conversation .","however, some models of topic segmentation ignore the social aspect of conversations.",0.0,0.4508928571428571
448,"recently , new reordering strategies have been proposed in the literature on smt such as the reordering of each source sentence to match the word order in the corresponding target sentence , see kanthak et al and crego et al .","recently , new reordering strategies have been proposed such as the reordering of each source sentence to match the word order in the corresponding target sentence , see kanthak et al and .","smt is currently being discussed as one of the new reordering strategies, and the reordering of a sentence according to the same criteria has been proposed.",0.0,0.3884615384615385
449,"in this study , we examined factors hypothesized to influence the propagation of words through a community of speakers , focusing on anglicisms in a german hip hop discussion .",we therefore study which factors contribute to the uptake of ( hip hop-related ) anglicisms in an online community of german hip hop fans over a span of 11 years .,"in this study, we use different domains, such as anglicism, language disambiguation, and r, a group of speakers to study the propagation of words through anglism to",0.0,0.1446945337620579
450,"for the model , we introduce three novel fine-grained relations .",we introduce a novel graph that incorporates three fine-grained relations .,"for the model, we introduce three novel fine-grained relations which could be found among the genitalities of an epiphany.",0.0,0.4770408163265306
451,our experiments show that our model achieves better accuracy than existing supervised and semi-supervised models .,experimental results show that our model outperforms state-of-the-art methods in both the supervised and semi-supervised settings .,our experiments show that our model achieves better accuracy than existing supervised and semi-supervised models.,0.2150707001701407,0.4608912721893491
452,"paraphrase identification is the problem to determine whether two sentences have the same meaning , and is the objective of the task 1 of semeval 2015 workshop ( cite-p-14-3-19 ) .",paraphrase identification is the task of judging if two texts express the same or very similar meaning .,"paraphrase identification is the task of identifying two sentences with a broader meaning, and a more complex task.",0.2610490903329069,0.37817121150454486
453,reiter and frank use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic nps .,reiter and frank exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific nps .,reiter and frank train a supervised classifier for identifying generic nps using a wide range of syntactic and semantic features.,0.0,0.4065573770491804
454,i show how this can be done on an example of the classical k-dnf learner .,"below , i show how this can be done by extending a k-dnf 4 learner of to a paradigm-learner .","for the learning of the k-dnf model, i show how this can be done on an example of the classical k-dnf learner.",0.3152861344254501,0.5669594226579521
455,"first , we add a transition to an existing non-projective parsing algorithm , so it can perform either projective or non-projective parsing .","to reduce the search space , we add a transition to an existing non-projective parsing algorithm .","we first do not rely on a transitional non-projective parsing algorithm, for example.",0.0,0.41806175595238104
456,from this we extracted grammar rules following the technique described in cohn and lapata .,to demonstrate this we have extracted paraphrase rules from our annotations using the grammar induction algorithm from cohn and lapata .,"by using the cohn re-joint model, we restructured grammar rules in two sections using grammar models.",0.0,0.1690821256038647
457,cite-p-18-3-7 presented a general framework to expand the short and sparse text by appending topic .,cite-p-18-1-11 proved that leveraging topics at multiple granularity can model short texts more precisely .,cite-p-18-3-7 presented a new framework to explore the short and sparse text by appending topic.,0.0,0.16556291390728475
458,passages are clustered using a combination of hierarchical clustering and n-bin classification .,the remaining passages are clustered using a combination of hierarchical clustering and n-bin classification .,"by applying hierarchical clustering as the classification of passages, the clustering method was a complex task by generating the number of text types into a segment.",0.0,0.27607361963190186
459,with the aid of this tool a domain expert was able to drastically reduce her model building time from months to two days .,"with the aid of this tool , a domain expert reduced her model building time from months to two days .",with this tool a domain expert enables a 2-day session-based model from months to days in order to simplify her model building process.,0.0,0.569967423589154
460,in this paper we show the effectiveness of partial-label learning in digesting the encoded knowledge from wikipedia data .,in this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised chinese word segmentation .,in this paper we look at the importance of partial-label learning for assessing the validity of partial-label learning of the wikipedia data.,0.12989750322243596,0.28797975102019735
461,word embedding provides an unique property to capture semantics and syntactic information of different words .,"word embedding techniques aim to use continuous low-dimension vectors representing the features of the words , captured in context .","word embeddings are provided with semantic information about words, as well as words in context.",0.0,0.3221796332746296
462,zelenko et al proposed extracting relations by computing kernel functions between parse trees .,"zelenko et al proposed a kernel between two parse trees , which recursively matches nodes from roots to leaves in a top-down manner .",zelenko et al proposed extracting relations by computing kernel functions between parse trees.,0.11454112651433533,0.3577563070316694
463,"as textual features , we use the pretrained google news word embeddings , obtained by training the skip-gram model with negative sampling .","in addition to pos-tags , we use the pre-trained word embeddings available from google 5 trained using the skip-gram objective as input features for our convolutional neural network .","we use the pretrained word embeddings of the google news word embeddings, a pre-trained sample using the skipgram model.",0.0,0.31504601137096294
464,random forest is an ensemble method that learns many classification trees and predicts an aggregation of their result .,the random forest algorithm is a decision tree algorithm which uses multiple random trees to vote for an overall classification of the given input .,random forest is a tagging algorithm that gathers a wide variety of semantic knowledge from individual entrants.,0.0,0.1440329218106996
465,rationales are never provided during training .,rationales are never given during training .,there are no rationales for implementing these rationales.,0.0,0.20833333333333331
466,the most prominent approaches include the karma system and the att-meta project .,these include the karma system and the att-meta project .,the most prominent approaches include the karma system and the att-meta project.,0.6479121525090147,0.8731871029605658
467,we extracted scfg rules from the parallel corpus using the standard heuristics and filtering strategies .,we extract hierarchical rules from the aligned parallel texts using the constraints developed by chiang .,we extracted scfg rules from the parallel corpus using the standard heuristics and filtering strategies.,0.0,0.5142746913580247
468,"in this work , we investigate the use of rule markov models in the context of tree-856 to-string translation .","in this paper , we incorporate the mers model into a stateof-the-art linguistically syntax-based smt model , the tree-to-string alignment template model .","in this work, we explore the use of rule markov models in the context of tree-856 to-string translation.",0.0,0.1762114537444934
469,"for the evaluation , we use the same measures as brent and goldwater et al , namely segmentation precision , recall and f-score .","for the evaluation , we used the same measures as brent , venkataraman and goldwater , namely token precision , recall and f-score .","for the evaluation, we use the same measures as brent and goldwater et al, namely segmentation precision, recall and f-score.",0.5356850921756573,0.864795918367347
470,"in this demo , we introduce need4tweet , a twitterbot for a combined system for nee and ned in tweets .","in this demo paper , we present need4tweet , a twitterbot for named entity extraction ( nee ) and disambiguation ( ned ) for tweets .","in this demo, we introduce need4tweet, a social media platform for social networking, and social networks.",0.0,0.3144725710939024
471,"islam and inkpen proposed a corpus-based sentence similarity measure as a function of string similarity , word similarity and common word order similarity .","for example , in they proposed a corpus-based sentence similarity measure as a function of string similarity , word similarity and common word order similarity .","islam and inkpen proposed a corpus-based sentence similarity measure based on string similarity, word similarity and common word order similarity.",0.5628595360358647,0.659737240484429
472,"this representation consists of two facets : a segmentation into minimal semantic units , and a labeling of some of those units with semantic classes .",this representation is the basis for the lexical-semantic level that is included in the kr component .,"this representation consists of two facets : a segmentation into minimal semantic units, and a labeling of some of those units with semantic classes.",0.0,0.1427684667908132
473,luong et al adapted an nmt model trained on general domain data with further training on in-domain data only .,"luong and manning , 2015 ) adapts an already existing nmt system to a new domain by further training on the in-domain data only .",luong et al and rds adapted the nmt model based on general domain data.,0.0,0.18749999999999997
474,evaluation shows that 98 . 3 % of distractors are reliable when generated by our method .,the result shows that 98.3 % of distractors generated by our methods are reliable .,evaluation shows that 98. 3 % of distractors are reliable when generated using our method.,0.0,0.6649630274032188
475,we explore whether using coreference can improve the learning process .,we explore whether coreference can improve the learning process .,we explore whether utilizing coreference can improve the learning process.,0.7016879391277371,0.9861386138613862
476,"with the help of the phrasal lexical disambiguation model , we build three models : a context-sensitive n-gram prediction model , a paraphrase suggestion model , and a translation model .","similar to their work , we further integrate the multi-word phrasal lexical disambiguation model to the n-gram prediction model , paraphrase model and translation model of our system .",in this study we construct a context-sensitive n-gram pre-trained lexical disambiguation model.,0.0,0.15561638280616383
477,"systems , we plan to design a supervised srl system .","so , we plan to design a supervised srl system too .",we proposed a supervised srl system in a supervised environment.,0.23708987804092638,0.4295051353874883
478,"our experimental results show that our proposed sentence type tagging method works very well , even for the minority categories .",our experimental results show that our proposed approach performs well for sentence dependency tagging .,we demonstrated that a sentence type tagging method works very well for all minority category.,0.0,0.2823767476085357
479,"in addition , we have compared our approach with other based on feature translation .","in addition , we have compared the results with a system which translates selected document features .","in addition, we compare our approach to other based on feature translation.",0.15172455787008743,0.4205651197604791
480,"cohn and lapata , 2007 ) cast the sentence compression problem as a tree-to-tree rewriting task .",cohn and lapata formulated sentence compression as a tree-to-tree rewrite problem .,"cohn and lapata, 2007 ) show that sentence compression is a problem in both writing and a context.",0.0,0.4156494140625
481,"multi-task learning has resulted in successful systems for various nlp tasks , especially in cross-lingual settings .","in particular , the vector-space word representations learned by a neural network have been shown to successfully improve various nlp tasks .",multi-task learning has given the blp tools a tremendous advantage for developing multi-task systems.,0.0,0.11737089201877933
482,kim and hovy and bethard et al examine the usefulness of semantic roles provided by framenet 1 for both oh and opinion target extraction .,bethard et al and kim and hovy explore the usefulness of semantic roles provided by framenet for both opinion holder and opinion target extraction .,kim and hovy and bethard et al use the framenet data to estimate what types of role you expect to use.,0.0,0.4389957597681049
483,the decoder is implemented with weighted finite state transducers using standard operations available in the openfst libraries .,our decoder is implemented as a cascade of weighted finite-state transducers using the functionalities of the openfst library .,the decoder is an integral feature in the finite state-based library.,0.0,0.2792956891317547
484,"fung and cheung , 2004 , for instance , present the first exploration of very nonparallel corpora using a document similarity measure based on bilingual lexical matching defined over mutual information scores on word pairs .","fung and cheung present the first exploration of very non-parallel corpora , using a document similarity measure based on bilingual lexical matching defined over mutual information scores on word pairs .","fung and cheung, 2004, proposed a lexical arrangement measure based on bilingual matching, which is applied to word pairs based on lexical characteristics of nlp.",0.13993407876150768,0.4099025974025974
485,"in an experimental evaluation on the test-set that was used in koehn et al we show that for examples that are in coverage of the grammar-based system , we can achieve stateof-the-art quality on n-gram based evaluation measures .","under the nist measure , we achieve results in the range of the state-of-the-art phrase-based system of koehn et al for in-coverage examples of the lfgbased system .","in this work, we use a classifier that evaluates the grammar-based system by applying state-of-the-art measures on the grammar-based evaluations provided.",0.0,0.21234567901234572
486,evaluation shows that docchat is a perfect complement for chatbot engines .,"these make docchat as a general response generation solution to chatbots , with high adaptation capability .",evaluation shows that docchat is the perfect addition for chatbot engines.,0.0,0.09090909090909091
487,autotutor eschews the pattern-based approach entirely in favor of a bow lsa approach .,autotutor eschews the pattern-based approach entirely in favor of a bag-of-words lsa approach .,autotutor eschews the pattern-based approach entirely in favor of a bow lsa approach.,0.7825422900366437,0.9268808114961962
488,le and mikolov introduce paragraph vector to learn document representation from semantics of words .,"le and mikolov introduced paragraph-level vectors , a fixed-length feature representations for variable-length texts .",the words based vectors are designed to learn the semantic semantics of words by using paragraphs.,0.0,0.0986842105263158
489,"through extensive experiments on real-world datasets , we find that neuraldater significantly outperforms state-of-the-art baseline .","through extensive experiments on real-world datasets , we demonstrate the effectiveness of neuraldater over existing state-of-the-art approaches .",we found that neuraldater significantly outperforms state-of-the-art baselines via extensive experiments on real-world datasets.,0.2333020571047663,0.46488107693380754
490,and thus predicting and recovering empty categories can be cast as a tree annotating problem .,a wide variety of language problems can be treated as or cast into a tree annotating problem .,"to address the problem of tree annotating, we must consider predicting and recovering empty categories.",0.0,0.2382060831781502
491,"in the no context , partial profile and full profile conditions , annotators often selected the ¡° neutral ¡± option ( x-axis ) when the model inferred .","we see that in the no context , partial profile and full profile conditions , annotators often selected the ¡°neutral¡± option ( x-axis ) when the model inferred the true label was ¡°clinton¡± or ¡°trump¡± ( y-axis ) .","in the no context, partial profile and full profile conditions, annotators often selected the ° neutral option ( x-axis ) when the model inferred.",0.5309143172689262,0.6248042328042329
492,an alternation is a pattern in which a number of words share the same relationship between a pair of senses .,an alternation is a pattern in which a number of words share the same relationship between ? a pair of senses .,an alternation is a series of random terms with a number of words which corresponds to the same meaning.,0.25452306291173127,0.4606156017588621
493,barzilay and mckeown extracted both single-and multiple-word paraphrases from a sentence-aligned corpus for use in multi-document summarization .,barzilay and mckeown acquire paraphrases from a monolingual parallel corpus using a co-training algorithm .,barzilay and mckeown extracted both single-and multi-word paraphrases from a sentence-aligned corpus for use in multidocument summarization.,0.0,0.5010893246187365
494,"in this work , we improve the robustness of encoder representations against noisy perturbations with adversarial learning .","in this paper , we propose to improve the robustness of nmt models with adversarial stability training .","in this work, we develop a robustness of encoder representations with adversarial learning.",0.0,0.503954802259887
495,"in this thesis , we propose and evaluate novel text quality .","in this thesis , we propose computable measures to capture genre-specific text quality .",in this paper we aim to measure and evaluate novel text quality.,0.0,0.505283273381295
496,cite-p-20-1-22 used sequence labeling model ( crf ) for normalizing deletion-based abbreviation .,cite-p-20-1-22 used a crf sequence modeling approach for deletion-based abbreviations .,cite-p-20-1-6-22 used codifer-based sequence labeling model ( crf ) for normalizing deletion-based abbreviation.,0.194086232706272,0.5586283185840708
497,"however , the size of the used corpora still leads to data sparseness and the extraction procedure can therefore require extensive smoothing .","therefore , the size of the corpora used in some previous approaches leads to data sparseness , and the extraction procedure can therefore require extensive smoothing .","the size of the used corpora leads to data sparseness, while the extraction process can therefore require extensive smoothing.",0.4148295558123954,0.7083648115504072
498,"in this paper , we develop a novel behavior-based assessment using human language .","in this paper , we develop a temporal orientation measure based on language in social media .","in this paper, we develop a novel behavior-based assessment using human language.",0.37164774823109187,0.5152657647667627
499,"in section 4 , we describe tools allowing to efficiently access wikipedia ’ s edit history .","thus , in section 4 , we present a tool to efficiently access wikipedia ’ s edit history .","in section 4, we describe a novel tool that allows you to efficiently access the wikipedia text.",0.28051550320162943,0.5921052631578947
